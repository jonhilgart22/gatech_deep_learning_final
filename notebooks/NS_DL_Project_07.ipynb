{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_Project_07.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMcAiDVH9/mCq/NRatomK6f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7kFoifkmuQeO","executionInfo":{"status":"ok","timestamp":1606833564594,"user_tz":300,"elapsed":2672,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"028ff132-3d50-47b3-ed6c-515bac81630b"},"source":["import torch\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(device)\n","\n","!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["cuda\n","Tue Dec  1 14:39:25 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    28W / 250W |     10MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OuqHjW02ufcp","executionInfo":{"status":"ok","timestamp":1606833569640,"user_tz":300,"elapsed":1294,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"b32f38e3-8523-4fe2-efb8-ee897e324e05"},"source":["!git clone https://github.com/nsusanj/temp.git"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'temp'...\n","remote: Enumerating objects: 24, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (24/24), done.\u001b[K\n","remote: Total 24 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (24/24), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vShHTG_yuhcR","executionInfo":{"status":"ok","timestamp":1606833600165,"user_tz":300,"elapsed":28732,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"f47edaa2-d54e-425d-cb75-a54c58c20be5"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lndb5VaBujww","executionInfo":{"status":"ok","timestamp":1606833606258,"user_tz":300,"elapsed":3386,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"afe15f86-8b1b-49af-c4f7-4333fdae8abd"},"source":["!git clone https://github.com/adapter-hub/adapter-transformers.git\n","%cd adapter-transformers"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Cloning into 'adapter-transformers'...\n","remote: Enumerating objects: 29, done.\u001b[K\n","remote: Counting objects: 100% (29/29), done.\u001b[K\n","remote: Compressing objects: 100% (24/24), done.\u001b[K\n","remote: Total 42519 (delta 10), reused 8 (delta 2), pack-reused 42490\u001b[K\n","Receiving objects: 100% (42519/42519), 23.88 MiB | 27.69 MiB/s, done.\n","Resolving deltas: 100% (30833/30833), done.\n","/content/adapter-transformers\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wbl88J40ullQ","executionInfo":{"status":"ok","timestamp":1606833626211,"user_tz":300,"elapsed":18965,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"df694c74-e688-4d57-a61c-f2052789077b"},"source":["!pip install .\n","!pip install sklearn"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Processing /content/adapter-transformers\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.1.0) (1.18.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.1.0) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.1.0) (20.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.1.0) (3.0.12)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 5.9MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.1.0) (4.41.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.1.0) (3.12.4)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 31.6MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 42.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.1.0) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from adapter-transformers==1.1.0) (0.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers==1.1.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers==1.1.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers==1.1.0) (2020.11.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers==1.1.0) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->adapter-transformers==1.1.0) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->adapter-transformers==1.1.0) (2.4.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->adapter-transformers==1.1.0) (50.3.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers==1.1.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers==1.1.0) (0.17.0)\n","Building wheels for collected packages: adapter-transformers\n","  Building wheel for adapter-transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for adapter-transformers: filename=adapter_transformers-1.1.0-cp36-none-any.whl size=1325191 sha256=6a947231c979cdfb6eea34ed6623eac3d7b219c917d4fa7b33688593132309d8\n","  Stored in directory: /root/.cache/pip/wheels/69/f2/bd/f0720d145cbd02c42f9d833e06bb319e9ee518b24ebaa092e1\n","Successfully built adapter-transformers\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=8c1e52ae7e7852a20defa60cb4b1df444731f35b12a0b5d18e536fd784e2ff20\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, tokenizers, sacremoses, adapter-transformers\n","Successfully installed adapter-transformers-1.1.0 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.17.0)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uScMH2bouoQg","executionInfo":{"status":"ok","timestamp":1606833632910,"user_tz":300,"elapsed":945,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"2b59dbd0-73b7-45cd-ca73-2746be5b2f5d"},"source":["%cd ../temp/adapticons\n","%mkdir datasets\n","%cd datasets/\n","%mkdir hyper\n","%cd hyper"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons\n","/content/temp/adapticons/datasets\n","/content/temp/adapticons/datasets/hyper\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJ9FEc9Wu4FY","executionInfo":{"status":"ok","timestamp":1606833658077,"user_tz":300,"elapsed":2881,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"6aba027e-1a82-40f1-a1d5-c880e94bb655"},"source":["!curl -Lo train.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/hyperpartisan_news/train.jsonl\n","!curl -Lo dev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/hyperpartisan_news/dev.jsonl\n","!curl -Lo test.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/hyperpartisan_news/test.jsonl"],"execution_count":10,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 1708k  100 1708k    0     0  1885k      0 --:--:-- --:--:-- --:--:-- 1883k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  197k  100  197k    0     0   324k      0 --:--:-- --:--:-- --:--:--  324k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  241k  100  241k    0     0   337k      0 --:--:-- --:--:-- --:--:--  337k\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NqXVeQpqu8ig","executionInfo":{"status":"ok","timestamp":1606833662498,"user_tz":300,"elapsed":306,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"99900974-14b8-4913-d390-a867580ada0d"},"source":["%cd ../../modeling"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons/modeling\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p60rdWnHvDog","executionInfo":{"status":"ok","timestamp":1606834003827,"user_tz":300,"elapsed":325749,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"389b3220-9107-43b2-912e-572411c4832c"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/hyper/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/tapt_hyper/newtrain/ \\\n","  --task_name HYPER \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --model_name_or_path ../../../gdrive/MyDrive/tapt_hyper/ \\\n","  --metric macro \\"],"execution_count":12,"outputs":[{"output_type":"stream","text":["2020-12-01 14:41:21.082192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/01/2020 14:41:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/01/2020 14:41:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/tapt_hyper/newtrain/', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec01_14-41-22_8ccda2734bf9', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/tapt_hyper/newtrain/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/tapt_hyper/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"HYPER\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/tapt_hyper/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/tapt_hyper/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/tapt_hyper/' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/tapt_hyper/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/tapt_hyper/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/tapt_hyper/vocab.json\n","loading file ../../../gdrive/MyDrive/tapt_hyper/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/tapt_hyper/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/tapt_hyper/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/tapt_hyper/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/tapt_hyper/ were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/tapt_hyper/ and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'HYPER' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}}.\n","***** Running training *****\n","  Num examples = 516\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 330\n","100% 330/330 [04:47<00:00,  1.43it/s]\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'epoch': 10.0}\n","100% 330/330 [04:47<00:00,  1.15it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/tapt_hyper/newtrain/\n","Configuration saved in ../../../gdrive/MyDrive/tapt_hyper/newtrain/config.json\n","Model weights saved in ../../../gdrive/MyDrive/tapt_hyper/newtrain/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/01/2020 14:46:40 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 64\n","  Batch size = 8\n","100% 8/8 [00:01<00:00,  6.83it/s]\n","12/01/2020 14:46:42 - INFO - __main__ -   ***** Eval results HYPER *****\n","12/01/2020 14:46:42 - INFO - __main__ -     eval_loss = 0.8070167303085327\n","12/01/2020 14:46:42 - INFO - __main__ -     eval_accuracy = 0.828125\n","12/01/2020 14:46:42 - INFO - __main__ -     eval_f1 = 0.8150774888363541\n","12/01/2020 14:46:42 - INFO - __main__ -     eval_precision = 0.8355481727574751\n","12/01/2020 14:46:42 - INFO - __main__ -     eval_recall = 0.8066801619433198\n","12/01/2020 14:46:42 - INFO - __main__ -     epoch = 10.0\n","12/01/2020 14:46:42 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 65\n","  Batch size = 8\n","100% 9/9 [00:01<00:00,  7.18it/s]\n","12/01/2020 14:46:43 - INFO - __main__ -   ***** Test results HYPER *****\n","12/01/2020 14:46:43 - INFO - __main__ -     eval_loss = 0.3208296000957489\n","12/01/2020 14:46:43 - INFO - __main__ -     eval_accuracy = 0.9230769230769231\n","12/01/2020 14:46:43 - INFO - __main__ -     eval_f1 = 0.9181153943058704\n","12/01/2020 14:46:43 - INFO - __main__ -     eval_precision = 0.9418604651162791\n","12/01/2020 14:46:43 - INFO - __main__ -     eval_recall = 0.9074074074074074\n","12/01/2020 14:46:43 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8oY-ZdKywCbM","executionInfo":{"status":"ok","timestamp":1606834145606,"user_tz":300,"elapsed":337,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"61f54f17-0544-41b2-b490-a89160f58e4a"},"source":["%cd ../datasets/\n","%mkdir citation\n","%cd citation/"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons/datasets\n","/content/temp/adapticons/datasets/citation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uJmWS3NKx0fE","executionInfo":{"status":"ok","timestamp":1606834172804,"user_tz":300,"elapsed":2392,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"39d7623b-080d-4953-a060-59a21431a2b7"},"source":["!curl -Lo train.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/train.jsonl\n","!curl -Lo dev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/dev.jsonl\n","!curl -Lo test.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/test.jsonl"],"execution_count":14,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  454k  100  454k    0     0   498k      0 --:--:-- --:--:-- --:--:--  498k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 30138  100 30138    0     0  67122      0 --:--:-- --:--:-- --:--:-- 66973\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 38637  100 38637    0     0  82381      0 --:--:-- --:--:-- --:--:-- 82381\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVWbohHtx6oB","executionInfo":{"status":"ok","timestamp":1606834183537,"user_tz":300,"elapsed":350,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"018a009c-d64a-4ecb-9040-69b22ad0569a"},"source":["%cd ../../modeling"],"execution_count":15,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons/modeling\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzt5PYPfx9vu","executionInfo":{"status":"ok","timestamp":1606835264993,"user_tz":300,"elapsed":975531,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"5f1438bf-6ee3-40e7-8596-42a0165c0fdf"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/citation/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/tapt_citation/newtrain/ \\\n","  --task_name citation \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --model_name_or_path ../../../gdrive/MyDrive/tapt_citation/ \\\n","  --metric macro \\"],"execution_count":16,"outputs":[{"output_type":"stream","text":["2020-12-01 14:51:31.903534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/01/2020 14:51:33 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/01/2020 14:51:33 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/tapt_citation/newtrain/', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec01_14-51-33_8ccda2734bf9', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/tapt_citation/newtrain/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/tapt_citation/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"citation\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/tapt_citation/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/tapt_citation/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/tapt_citation/' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/tapt_citation/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/tapt_citation/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/tapt_citation/vocab.json\n","loading file ../../../gdrive/MyDrive/tapt_citation/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/tapt_citation/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/tapt_citation/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/tapt_citation/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/tapt_citation/ were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/tapt_citation/ and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'citation' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}}.\n","***** Running training *****\n","  Num examples = 1688\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1060\n","{'loss': 0.6816025390625, 'learning_rate': 1.0566037735849058e-05, 'epoch': 4.716981132075472}\n","{'loss': 0.1214857177734375, 'learning_rate': 1.1320754716981133e-06, 'epoch': 9.433962264150944}\n","100% 1060/1060 [15:37<00:00,  1.31it/s]\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'epoch': 10.0}\n","100% 1060/1060 [15:37<00:00,  1.13it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/tapt_citation/newtrain/\n","Configuration saved in ../../../gdrive/MyDrive/tapt_citation/newtrain/config.json\n","Model weights saved in ../../../gdrive/MyDrive/tapt_citation/newtrain/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/01/2020 15:07:39 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 114\n","  Batch size = 8\n","100% 15/15 [00:02<00:00,  7.15it/s]\n","12/01/2020 15:07:42 - INFO - __main__ -   ***** Eval results citation *****\n","12/01/2020 15:07:42 - INFO - __main__ -     eval_loss = 0.9015637040138245\n","12/01/2020 15:07:42 - INFO - __main__ -     eval_accuracy = 0.8070175438596491\n","12/01/2020 15:07:42 - INFO - __main__ -     eval_f1 = 0.7161357500030268\n","12/01/2020 15:07:42 - INFO - __main__ -     eval_precision = 0.721448023653906\n","12/01/2020 15:07:42 - INFO - __main__ -     eval_recall = 0.7397027172450902\n","12/01/2020 15:07:42 - INFO - __main__ -     epoch = 10.0\n","12/01/2020 15:07:42 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 139\n","  Batch size = 8\n","100% 18/18 [00:02<00:00,  7.02it/s]\n","12/01/2020 15:07:44 - INFO - __main__ -   ***** Test results citation *****\n","12/01/2020 15:07:44 - INFO - __main__ -     eval_loss = 0.8939087986946106\n","12/01/2020 15:07:44 - INFO - __main__ -     eval_accuracy = 0.7985611510791367\n","12/01/2020 15:07:44 - INFO - __main__ -     eval_f1 = 0.6869030993651054\n","12/01/2020 15:07:44 - INFO - __main__ -     eval_precision = 0.6973178994918126\n","12/01/2020 15:07:44 - INFO - __main__ -     eval_recall = 0.6788856214208328\n","12/01/2020 15:07:44 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"npqkZXOoyXsI","executionInfo":{"status":"ok","timestamp":1606835380094,"user_tz":300,"elapsed":354,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"5d82377b-6f1d-4fc9-ebea-5345153f9e13"},"source":["%cd ../datasets/\n","%mkdir sciie\n","%cd sciie/"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons/datasets\n","/content/temp/adapticons/datasets/sciie\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQf3lN3X13_X","executionInfo":{"status":"ok","timestamp":1606835384452,"user_tz":300,"elapsed":3289,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"93f79bda-f452-4f2d-88a7-7faddbf7dc71"},"source":["!curl -Lo train.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/sciie/train.jsonl\n","!curl -Lo dev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/sciie/dev.jsonl\n","!curl -Lo test.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/sciie/test.jsonl"],"execution_count":18,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  804k  100  804k    0     0   844k      0 --:--:-- --:--:-- --:--:--  843k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  115k  100  115k    0     0   171k      0 --:--:-- --:--:-- --:--:--  170k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  243k  100  243k    0     0   325k      0 --:--:-- --:--:-- --:--:--  325k\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CAiNNh5P1-0G","executionInfo":{"status":"ok","timestamp":1606835385361,"user_tz":300,"elapsed":365,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"0194721e-370c-460d-f289-fcb6cbd19430"},"source":["%cd ../../modeling"],"execution_count":19,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons/modeling\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzeYREbk1_Xl","executionInfo":{"status":"ok","timestamp":1606837320488,"user_tz":300,"elapsed":1846461,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"199b84a0-e13d-4f20-d6d5-ad87d19b87b0"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/sciie/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/tapt_sciie/newtrain/ \\\n","  --task_name sciie \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --model_name_or_path ../../../gdrive/MyDrive/tapt_sciie/ \\\n","  --metric macro \\"],"execution_count":20,"outputs":[{"output_type":"stream","text":["2020-12-01 15:11:16.453622: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/01/2020 15:11:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/01/2020 15:11:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/tapt_sciie/newtrain/', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec01_15-11-18_8ccda2734bf9', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/tapt_sciie/newtrain/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/tapt_sciie/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"sciie\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/tapt_sciie/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/tapt_sciie/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/tapt_sciie/' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/tapt_sciie/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/tapt_sciie/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/tapt_sciie/vocab.json\n","loading file ../../../gdrive/MyDrive/tapt_sciie/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/tapt_sciie/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/tapt_sciie/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/tapt_sciie/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/tapt_sciie/ were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/tapt_sciie/ and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'sciie' with config {'head_type': 'classification', 'num_labels': 7, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6}}.\n","***** Running training *****\n","  Num examples = 3219\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2020\n","{'loss': 0.8125056762695313, 'learning_rate': 1.504950495049505e-05, 'epoch': 2.4752475247524752}\n","{'loss': 0.1801171875, 'learning_rate': 1.00990099009901e-05, 'epoch': 4.9504950495049505}\n","{'loss': 0.04825433349609375, 'learning_rate': 5.148514851485149e-06, 'epoch': 7.425742574257426}\n","{'loss': 0.0246944580078125, 'learning_rate': 1.9801980198019803e-07, 'epoch': 9.900990099009901}\n","100% 2020/2020 [29:44<00:00,  1.46it/s]\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'epoch': 10.0}\n","100% 2020/2020 [29:44<00:00,  1.13it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/tapt_sciie/newtrain/\n","Configuration saved in ../../../gdrive/MyDrive/tapt_sciie/newtrain/config.json\n","Model weights saved in ../../../gdrive/MyDrive/tapt_sciie/newtrain/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/01/2020 15:41:33 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 455\n","  Batch size = 8\n","100% 57/57 [00:08<00:00,  6.74it/s]\n","12/01/2020 15:41:42 - INFO - __main__ -   ***** Eval results sciie *****\n","12/01/2020 15:41:42 - INFO - __main__ -     eval_loss = 0.5612578392028809\n","12/01/2020 15:41:42 - INFO - __main__ -     eval_accuracy = 0.9054945054945055\n","12/01/2020 15:41:42 - INFO - __main__ -     eval_f1 = 0.873622749335006\n","12/01/2020 15:41:42 - INFO - __main__ -     eval_precision = 0.8844807851584283\n","12/01/2020 15:41:42 - INFO - __main__ -     eval_recall = 0.8635858050477992\n","12/01/2020 15:41:42 - INFO - __main__ -     epoch = 10.0\n","12/01/2020 15:41:42 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 974\n","  Batch size = 8\n","100% 122/122 [00:18<00:00,  6.75it/s]\n","12/01/2020 15:42:00 - INFO - __main__ -   ***** Test results sciie *****\n","12/01/2020 15:42:00 - INFO - __main__ -     eval_loss = 0.7558451890945435\n","12/01/2020 15:42:00 - INFO - __main__ -     eval_accuracy = 0.8767967145790554\n","12/01/2020 15:42:00 - INFO - __main__ -     eval_f1 = 0.8072511892594179\n","12/01/2020 15:42:00 - INFO - __main__ -     eval_precision = 0.8059583354829835\n","12/01/2020 15:42:00 - INFO - __main__ -     eval_recall = 0.8095442078435402\n","12/01/2020 15:42:00 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gmSGPxlc2447"},"source":[""],"execution_count":null,"outputs":[]}]}