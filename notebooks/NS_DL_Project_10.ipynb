{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_Project_10.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPIT8sWEvaevAMbX5Z/uasJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOiqOct87PZg","executionInfo":{"status":"ok","timestamp":1607140045755,"user_tz":300,"elapsed":2818,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"e5938619-92fe-4bb0-8d34-24d33f699e66"},"source":["import torch\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(device)\n","\n","!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["cuda\n","Sat Dec  5 03:47:25 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    24W / 300W |     10MiB / 16130MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xBgY2moM7UZ5","executionInfo":{"status":"ok","timestamp":1607140070850,"user_tz":300,"elapsed":19166,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"0dd6eaa5-b7ff-4a20-8789-cb755b0b9d01"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9LXt7_fb7Wk4","executionInfo":{"status":"ok","timestamp":1607140218563,"user_tz":300,"elapsed":144892,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"a9776e37-d77d-42be-d7ed-62752a2d6f61"},"source":["!pip install git+https://github.com/kernelmachine/allennlp.git@4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7\n","!pip install transformers==2.4.1\n","!pip install pytorch-transformers==1.2.0\n","!git clone https://github.com/allenai/dont-stop-pretraining\n","\n","!pip install adapter-transformers"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/kernelmachine/allennlp.git@4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7\n","  Cloning https://github.com/kernelmachine/allennlp.git (to revision 4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7) to /tmp/pip-req-build-rcpchwio\n","  Running command git clone -q https://github.com/kernelmachine/allennlp.git /tmp/pip-req-build-rcpchwio\n","  Running command git checkout -q 4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (2.10.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (3.6.4)\n","Collecting responses>=0.7\n","  Downloading https://files.pythonhosted.org/packages/d5/71/4f04aed03ca35f2d02e1732ca6e996b2d7b40232fb7f1b58ff35f9a89b7b/responses-0.12.1-py2.py3-none-any.whl\n","Collecting parsimonious>=0.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (3.2.5)\n","Collecting conllu==1.3.1\n","  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (1.4.1)\n","Collecting flask-cors>=3.0.7\n","  Downloading https://files.pythonhosted.org/packages/69/7f/d0aeaaafb5c3c76c8d2141dbe2d4f6dca5d6c31872d4e5349768c1958abc/Flask_Cors-3.0.9-py2.py3-none-any.whl\n","Collecting overrides\n","  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n","Collecting spacy<2.2,>=2.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n","\u001b[K     |████████████████████████████████| 30.9MB 179kB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (0.22.2.post1)\n","Collecting jsonpickle\n","  Downloading https://files.pythonhosted.org/packages/ee/d5/1cc282dc23346a43aab461bf2e8c36593aacd34242bee1a13fa750db0cfe/jsonpickle-1.4.2-py2.py3-none-any.whl\n","Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (3.2.2)\n","Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (0.4.1)\n","Collecting pytorch-pretrained-bert>=0.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 51.8MB/s \n","\u001b[?25hCollecting word2number>=1.1\n","  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/9c/544396572c05841b7a2482c88be5dd54dcd18ba97abeb1e8d34daf921a54/boto3-1.16.30-py2.py3-none-any.whl (129kB)\n","\u001b[K     |████████████████████████████████| 133kB 51.3MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (1.7.0+cu101)\n","Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\u001b[K     |████████████████████████████████| 245kB 51.5MB/s \n","\u001b[?25hCollecting flaky\n","  Downloading https://files.pythonhosted.org/packages/43/0e/2f50064e327f41a1eb811df089f813036e19a64b95e33f8e9e0b96c2447e/flaky-3.7.0-py2.py3-none-any.whl\n","Collecting tensorboardX>=1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\u001b[K     |████████████████████████████████| 317kB 50.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (1.18.5)\n","Collecting pytorch-transformers==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n","\u001b[K     |████████████████████████████████| 163kB 48.0MB/s \n","\u001b[?25hCollecting numpydoc>=0.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n","\u001b[?25hCollecting gevent>=1.3.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/92/b80b922f08f222faca53c8d278e2e612192bc74b0e1f0db2f80a6ee46982/gevent-20.9.0-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n","\u001b[K     |████████████████████████████████| 5.3MB 42.9MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (4.41.1)\n","Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (2.23.0)\n","Collecting ftfy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n","\u001b[K     |████████████████████████████████| 71kB 9.5MB/s \n","\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n","\u001b[K     |████████████████████████████████| 266kB 57.3MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (2018.9)\n","Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (1.1.2)\n","Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (0.5.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->allennlp===0.9.1-unreleased) (1.15.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (8.6.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (20.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (50.3.2)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (1.9.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (1.4.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (0.7.1)\n","Collecting urllib3>=1.25.10\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl (136kB)\n","\u001b[K     |████████████████████████████████| 143kB 54.4MB/s \n","\u001b[?25hCollecting thinc<7.1.0,>=7.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 49.9MB/s \n","\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n","  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n","Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (1.0.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (0.8.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (1.0.4)\n","Collecting preshed<2.1.0,>=2.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n","\u001b[K     |████████████████████████████████| 92kB 11.0MB/s \n","\u001b[?25hCollecting blis<0.3.0,>=0.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 48.6MB/s \n","\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (2.0.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp===0.9.1-unreleased) (0.17.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp===0.9.1-unreleased) (2.0.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (2.4.7)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp===0.9.1-unreleased) (2019.12.20)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 8.3MB/s \n","\u001b[?25hCollecting botocore<1.20.0,>=1.19.30\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/a3/1ee497faf994d180df5d14d456eef1ef46ca1ffce617816faa4ff8164608/botocore-1.19.30-py2.py3-none-any.whl (7.0MB)\n","\u001b[K     |████████████████████████████████| 7.0MB 39.8MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp===0.9.1-unreleased) (0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp===0.9.1-unreleased) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp===0.9.1-unreleased) (0.16.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp===0.9.1-unreleased) (3.12.4)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 45.7MB/s \n","\u001b[?25hRequirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.8.5)\n","Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.11.2)\n","Collecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d0/532e160c777b42f6f393f9de8c88abb8af6c892037c55e4d3a8a211324dd/greenlet-0.4.17-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n","\u001b[?25hCollecting zope.interface\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n","\u001b[K     |████████████████████████████████| 245kB 57.9MB/s \n","\u001b[?25hCollecting zope.event\n","  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp===0.9.1-unreleased) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp===0.9.1-unreleased) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp===0.9.1-unreleased) (2020.11.8)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp===0.9.1-unreleased) (0.2.5)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp===0.9.1-unreleased) (1.0.1)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp===0.9.1-unreleased) (7.1.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp===0.9.1-unreleased) (1.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp===0.9.1-unreleased) (3.4.0)\n","Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (0.16)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (0.7.12)\n","Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.2.4)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.6.1)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.0.0)\n","Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.9.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (20.4)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.2.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.1.1)\n","Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.1.4)\n","Building wheels for collected packages: allennlp\n","  Building wheel for allennlp (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for allennlp: filename=allennlp-0.9.1_unreleased-cp36-none-any.whl size=7535392 sha256=92c5206dbcab7d18da792eb68ef5646d1d6fcdd2294fbe20f552b318432a1e27\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-mdmcl9wm/wheels/16/b3/9b/fceece1cbc3a6ac0c759db090cb239c3f4cba5bb369bb933c3\n","Successfully built allennlp\n","Building wheels for collected packages: parsimonious, overrides, word2number, ftfy, jsonnet\n","  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=25881af0dd291ac80e992b2ac35b2cd1e8eab5fb4d1b8bd6ed1bc2acb2749fe8\n","  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10174 sha256=cd6a58f92c7b7185ef854147246fbaa502f36a793bf0de42962450a534346dfb\n","  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=691f42fb8c8f6b60a22ef575f81efbf8268c6cf566ea414fcac20a4fce2d8882\n","  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=c688f167cb20e1a91f170021f07bcea65e9019875593d63f082b8b5404054e4e\n","  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp36-cp36m-linux_x86_64.whl size=3388037 sha256=bcfde70c02e98c2ddc78e33ee87dfb26170c2bd6d44bae6a637a7a9d46f409cd\n","  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n","Successfully built parsimonious overrides word2number ftfy jsonnet\n","\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","Installing collected packages: urllib3, responses, parsimonious, conllu, flask-cors, overrides, plac, blis, preshed, thinc, spacy, jsonpickle, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, word2number, unidecode, flaky, tensorboardX, sentencepiece, pytorch-transformers, numpydoc, greenlet, zope.interface, zope.event, gevent, ftfy, jsonnet, allennlp\n","  Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Found existing installation: plac 1.1.3\n","    Uninstalling plac-1.1.3:\n","      Successfully uninstalled plac-1.1.3\n","  Found existing installation: blis 0.4.1\n","    Uninstalling blis-0.4.1:\n","      Successfully uninstalled blis-0.4.1\n","  Found existing installation: preshed 3.0.4\n","    Uninstalling preshed-3.0.4:\n","      Successfully uninstalled preshed-3.0.4\n","  Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed allennlp-0.9.1-unreleased blis-0.2.4 boto3-1.16.30 botocore-1.19.30 conllu-1.3.1 flaky-3.7.0 flask-cors-3.0.9 ftfy-5.8 gevent-20.9.0 greenlet-0.4.17 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-1.4.2 numpydoc-1.1.0 overrides-3.1.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.12.1 s3transfer-0.3.3 sentencepiece-0.1.94 spacy-2.1.9 tensorboardX-2.1 thinc-7.0.8 unidecode-1.1.1 urllib3-1.26.2 word2number-1.1 zope.event-4.5.0 zope.interface-5.2.0\n","Collecting transformers==2.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n","\u001b[K     |████████████████████████████████| 481kB 12.9MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (2.23.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (0.1.94)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 28.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (1.18.5)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (1.16.30)\n","Collecting tokenizers==0.0.11\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.1MB 58.3MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.1) (2020.11.8)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n","\u001b[K     |████████████████████████████████| 133kB 50.2MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.1) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1) (0.17.0)\n","Requirement already satisfied: botocore<1.20.0,>=1.19.30 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1) (1.19.30)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1) (0.10.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.30->boto3->transformers==2.4.1) (2.8.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=5838e303c1050d4a5377b08e1ae427c02f9d2c994856dbd8c1504f4c9274dc68\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","Installing collected packages: sacremoses, tokenizers, transformers, urllib3\n","  Found existing installation: urllib3 1.26.2\n","    Uninstalling urllib3-1.26.2:\n","      Successfully uninstalled urllib3-1.26.2\n","Successfully installed sacremoses-0.0.43 tokenizers-0.0.11 transformers-2.4.1 urllib3-1.25.11\n","Collecting pytorch-transformers==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n","\u001b[K     |████████████████████████████████| 184kB 12.7MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (1.16.30)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (0.0.43)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (1.7.0+cu101)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (1.18.5)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (0.1.94)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (4.41.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0) (0.10.0)\n","Requirement already satisfied: botocore<1.20.0,>=1.19.30 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0) (1.19.30)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0) (0.3.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (0.17.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (7.1.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (0.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (2020.11.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (3.0.4)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.30->boto3->pytorch-transformers==1.2.0) (2.8.1)\n","\u001b[31mERROR: allennlp 0.9.1-unreleased has requirement pytorch-transformers==1.1.0, but you'll have pytorch-transformers 1.2.0 which is incompatible.\u001b[0m\n","Installing collected packages: pytorch-transformers\n","  Found existing installation: pytorch-transformers 1.1.0\n","    Uninstalling pytorch-transformers-1.1.0:\n","      Successfully uninstalled pytorch-transformers-1.1.0\n","Successfully installed pytorch-transformers-1.2.0\n","Cloning into 'dont-stop-pretraining'...\n","remote: Enumerating objects: 439, done.\u001b[K\n","remote: Total 439 (delta 0), reused 0 (delta 0), pack-reused 439\u001b[K\n","Receiving objects: 100% (439/439), 566.01 KiB | 1.16 MiB/s, done.\n","Resolving deltas: 100% (232/232), done.\n","Collecting adapter-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/44/1370c187aba1349d56d6813ec4de54644d15e154983050f4923ce5455069/adapter_transformers-1.1.0-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 11.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (4.41.1)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 47.4MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (1.18.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (20.4)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (0.0.43)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (2019.12.20)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 57.9MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (0.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (2020.11.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (2.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->adapter-transformers) (50.3.2)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->adapter-transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->adapter-transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers) (0.17.0)\n","\u001b[31mERROR: transformers 2.4.1 has requirement tokenizers==0.0.11, but you'll have tokenizers 0.9.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: allennlp 0.9.1-unreleased has requirement pytorch-transformers==1.1.0, but you'll have pytorch-transformers 1.2.0 which is incompatible.\u001b[0m\n","Installing collected packages: tokenizers, sentencepiece, adapter-transformers\n","  Found existing installation: tokenizers 0.0.11\n","    Uninstalling tokenizers-0.0.11:\n","      Successfully uninstalled tokenizers-0.0.11\n","  Found existing installation: sentencepiece 0.1.94\n","    Uninstalling sentencepiece-0.1.94:\n","      Successfully uninstalled sentencepiece-0.1.94\n","Successfully installed adapter-transformers-1.1.0 sentencepiece-0.1.91 tokenizers-0.9.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sH63jWPm7iCH","executionInfo":{"status":"ok","timestamp":1607140464631,"user_tz":300,"elapsed":620,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"d199ad74-126d-4927-fad1-7bc80d9a5b30"},"source":["!ls"],"execution_count":5,"outputs":[{"output_type":"stream","text":["dont-stop-pretraining  gdrive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lwvv8e-v7kKI","executionInfo":{"status":"ok","timestamp":1607140470496,"user_tz":300,"elapsed":1354,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"04297e18-e967-4c48-c6ff-b4374a23400c"},"source":["!git clone https://github.com/nsusanj/temp.git"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Cloning into 'temp'...\n","remote: Enumerating objects: 24, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (24/24), done.\u001b[K\n","remote: Total 24 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (24/24), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o1zECSH679TZ","executionInfo":{"status":"ok","timestamp":1607051982713,"user_tz":300,"elapsed":747,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"464cb35d-1d69-415f-b1f1-29b9302e0403"},"source":["!ls gdrive/MyDrive/pre-trained"],"execution_count":6,"outputs":[{"output_type":"stream","text":["ag\t  citation_intent  hyperpartisan_news  rct-20k\n","chemprot  helpfulness\t   imdb\t\t       scierc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pk7jDXQy8H_d","executionInfo":{"status":"ok","timestamp":1607140481596,"user_tz":300,"elapsed":526,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"9c2f4080-1b29-47b2-b668-1f1a8f53c6cf"},"source":["%cd temp/adapticons/"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lq0lIndf9RJw","executionInfo":{"status":"ok","timestamp":1607047052940,"user_tz":300,"elapsed":488,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"6d670568-4c51-4413-83c9-e1226845a105"},"source":["!ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":["modeling  new_train_requirements.txt  README.md\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jtS7p0nI9SEZ","executionInfo":{"status":"ok","timestamp":1607051995639,"user_tz":300,"elapsed":8980,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"20b6f744-3868-4db2-f87d-c0c5705531f4"},"source":["%mkdir datasets\n","%cd datasets\n","%mkdir rct-20k\n","%cd rct-20k\n","!curl -Lo train.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/rct-20k/train.jsonl\n","!curl -Lo dev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/rct-20k/dev.jsonl\n","!curl -Lo test.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/rct-20k/test.jsonl\n","%cd ../.."],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons/datasets\n","/content/temp/adapticons/datasets/rct-20k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 34.7M  100 34.7M    0     0  9379k      0  0:00:03  0:00:03 --:--:-- 9381k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 5959k  100 5959k    0     0  2809k      0  0:00:02  0:00:02 --:--:-- 2809k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 5920k  100 5920k    0     0  2910k      0  0:00:02  0:00:02 --:--:-- 2910k\n","/content/temp/adapticons\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLeqj74c9wmQ","executionInfo":{"status":"ok","timestamp":1607052017809,"user_tz":300,"elapsed":773,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"2f517a7d-7aea-42e6-a851-caf61d12169c"},"source":["!ls datasets/rct-20k"],"execution_count":10,"outputs":[{"output_type":"stream","text":["dev.jsonl  test.jsonl  train.jsonl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IpJQGgkd92xI","executionInfo":{"status":"ok","timestamp":1607052020012,"user_tz":300,"elapsed":751,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"6ef1a96a-b93e-4b1d-b7fd-595b136dd3d1"},"source":["%cd modeling"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons/modeling\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gYaGq8Q9-1go","executionInfo":{"status":"ok","timestamp":1607052022275,"user_tz":300,"elapsed":459,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"076916b9-8c73-49bf-9b65-a7b95037ca32"},"source":["!ls ../../../gdrive/MyDrive/pre-trained"],"execution_count":12,"outputs":[{"output_type":"stream","text":["ag\t  citation_intent  hyperpartisan_news  rct-20k\n","chemprot  helpfulness\t   imdb\t\t       scierc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtP4Yk-VcZ0Q","executionInfo":{"status":"ok","timestamp":1607047141713,"user_tz":300,"elapsed":768,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"94a9bbbe-83e4-4234-846d-fed3687df797"},"source":["!ls ../datasets"],"execution_count":17,"outputs":[{"output_type":"stream","text":["rct-20k\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgE-kcGA9cqN","executionInfo":{"status":"ok","timestamp":1607123023601,"user_tz":300,"elapsed":70998948,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"dfd7fb2e-a658-4db0-aef5-b4661929fe14"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/rct-20k/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 8 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 1e-4 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/RCT_Fusion_chem_rct \\\n","  --task_name adapter_fusion \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --train_fusion \\\n","  --model_name_or_path roberta-base \\\n","  --adapter_config pfeiffer \\\n","  --metric micro \\\n","  --fusion_adapter_path1 ../../../gdrive/MyDrive/pre-trained/chemprot \\\n","  --fusion_adapter_path2 ../../../gdrive/MyDrive/pre-trained/rct-20k \\\n","  --overwrite_output_dir "],"execution_count":13,"outputs":[{"output_type":"stream","text":["2020-12-04 03:20:27.865984: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/04/2020 03:20:29 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/04/2020 03:20:29 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/RCT_Fusion_chem_rct', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec04_03-20-29_051f9bb23de1', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/RCT_Fusion_chem_rct', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","12/04/2020 03:20:29 - INFO - filelock -   Lock 140293078965160 acquired on /root/.cache/torch/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n","https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpib4h0qls\n","Downloading: 100% 481/481 [00:00<00:00, 408kB/s]\n","storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/torch/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","creating metadata file for /root/.cache/torch/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","12/04/2020 03:20:30 - INFO - filelock -   Lock 140293078965160 released on /root/.cache/torch/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"adapter_fusion\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/torch/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","12/04/2020 03:20:30 - INFO - filelock -   Lock 140293078962640 acquired on /root/.cache/torch/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n","https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpet448z6_\n","Downloading: 100% 899k/899k [00:00<00:00, 1.79MB/s]\n","storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/torch/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","creating metadata file for /root/.cache/torch/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","12/04/2020 03:20:31 - INFO - filelock -   Lock 140293078962640 released on /root/.cache/torch/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n","12/04/2020 03:20:31 - INFO - filelock -   Lock 140293078962640 acquired on /root/.cache/torch/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n","https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpiw_ftwu1\n","Downloading: 100% 456k/456k [00:00<00:00, 1.09MB/s]\n","storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/torch/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","creating metadata file for /root/.cache/torch/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","12/04/2020 03:20:32 - INFO - filelock -   Lock 140293078962640 released on /root/.cache/torch/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n","loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/torch/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/torch/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","12/04/2020 03:20:32 - INFO - filelock -   Lock 140293078962864 acquired on /root/.cache/torch/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n","https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp5oq3lpv5\n","Downloading: 100% 501M/501M [00:05<00:00, 87.4MB/s]\n","storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/torch/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","creating metadata file for /root/.cache/torch/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","12/04/2020 03:20:38 - INFO - filelock -   Lock 140293078962864 released on /root/.cache/torch/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/torch/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Loading module configuration from ../../../gdrive/MyDrive/pre-trained/chemprot/adapter_config.json\n","Adding adapter 'chemprot' of type 'text_lang'.\n","Loading module weights from ../../../gdrive/MyDrive/pre-trained/chemprot/pytorch_adapter.bin\n","Loading module configuration from ../../../gdrive/MyDrive/pre-trained/rct-20k/adapter_config.json\n","Adding adapter 'rct-20k' of type 'text_lang'.\n","Loading module weights from ../../../gdrive/MyDrive/pre-trained/rct-20k/pytorch_adapter.bin\n","12/04/2020 03:20:45 - INFO - __main__ -   Using adapter fusion with the following setup [['chemprot', 'rct-20k']]\n","12/04/2020 03:20:45 - INFO - __main__ -   Model adapters = [['chemprot', 'rct-20k']]\n","Adding head 'adapter_fusion' with config {'head_type': 'classification', 'num_labels': 5, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4}}.\n","***** Running training *****\n","  Num examples = 180040\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 225050\n","{'loss': 0.77021484375, 'learning_rate': 9.977782714952233e-05, 'epoch': 0.02221728504776716}\n","{'loss': 0.5685709228515625, 'learning_rate': 9.955565429904466e-05, 'epoch': 0.04443457009553432}\n","{'loss': 0.577902587890625, 'learning_rate': 9.933348144856699e-05, 'epoch': 0.06665185514330149}\n","{'loss': 0.5659815673828125, 'learning_rate': 9.911130859808931e-05, 'epoch': 0.08886914019106865}\n","{'loss': 0.5432578125, 'learning_rate': 9.888913574761165e-05, 'epoch': 0.11108642523883581}\n","{'loss': 0.532667724609375, 'learning_rate': 9.866696289713397e-05, 'epoch': 0.13330371028660298}\n","{'loss': 0.546919189453125, 'learning_rate': 9.84447900466563e-05, 'epoch': 0.15552099533437014}\n","{'loss': 0.5417197265625, 'learning_rate': 9.822261719617863e-05, 'epoch': 0.1777382803821373}\n","{'loss': 0.53853125, 'learning_rate': 9.800044434570096e-05, 'epoch': 0.19995556542990447}\n","{'loss': 0.52977392578125, 'learning_rate': 9.777827149522329e-05, 'epoch': 0.22217285047767163}\n","{'loss': 0.51502783203125, 'learning_rate': 9.755609864474562e-05, 'epoch': 0.24439013552543878}\n","{'loss': 0.5364365234375, 'learning_rate': 9.733392579426795e-05, 'epoch': 0.26660742057320597}\n","{'loss': 0.51385400390625, 'learning_rate': 9.711175294379027e-05, 'epoch': 0.2888247056209731}\n","{'loss': 0.52126513671875, 'learning_rate': 9.68895800933126e-05, 'epoch': 0.3110419906687403}\n","{'loss': 0.5209765625, 'learning_rate': 9.666740724283493e-05, 'epoch': 0.33325927571650743}\n","{'loss': 0.526916015625, 'learning_rate': 9.644523439235725e-05, 'epoch': 0.3554765607642746}\n","{'loss': 0.5192138671875, 'learning_rate': 9.622306154187959e-05, 'epoch': 0.3776938458120418}\n","{'loss': 0.48971875, 'learning_rate': 9.600088869140191e-05, 'epoch': 0.39991113085980895}\n","{'loss': 0.5241064453125, 'learning_rate': 9.577871584092425e-05, 'epoch': 0.4221284159075761}\n","{'loss': 0.5310703125, 'learning_rate': 9.555654299044657e-05, 'epoch': 0.44434570095534326}\n","{'loss': 0.533228515625, 'learning_rate': 9.533437013996889e-05, 'epoch': 0.4665629860031104}\n","{'loss': 0.5141591796875, 'learning_rate': 9.511219728949123e-05, 'epoch': 0.48878027105087757}\n","{'loss': 0.518236328125, 'learning_rate': 9.489002443901356e-05, 'epoch': 0.5109975560986447}\n","{'loss': 0.5231943359375, 'learning_rate': 9.466785158853589e-05, 'epoch': 0.5332148411464119}\n","{'loss': 0.5000595703125, 'learning_rate': 9.444567873805822e-05, 'epoch': 0.555432126194179}\n","{'loss': 0.527755859375, 'learning_rate': 9.422350588758055e-05, 'epoch': 0.5776494112419462}\n","{'loss': 0.491552734375, 'learning_rate': 9.400133303710287e-05, 'epoch': 0.5998666962897135}\n","{'loss': 0.5072255859375, 'learning_rate': 9.37791601866252e-05, 'epoch': 0.6220839813374806}\n","{'loss': 0.503693359375, 'learning_rate': 9.355698733614753e-05, 'epoch': 0.6443012663852478}\n","{'loss': 0.5036611328125, 'learning_rate': 9.333481448566985e-05, 'epoch': 0.6665185514330149}\n","{'loss': 0.5196787109375, 'learning_rate': 9.311264163519219e-05, 'epoch': 0.6887358364807821}\n","{'loss': 0.5029609375, 'learning_rate': 9.289046878471451e-05, 'epoch': 0.7109531215285492}\n","{'loss': 0.5021640625, 'learning_rate': 9.266829593423683e-05, 'epoch': 0.7331704065763164}\n","{'loss': 0.508734375, 'learning_rate': 9.244612308375917e-05, 'epoch': 0.7553876916240836}\n","{'loss': 0.523693359375, 'learning_rate': 9.222395023328149e-05, 'epoch': 0.7776049766718507}\n","{'loss': 0.50281640625, 'learning_rate': 9.200177738280382e-05, 'epoch': 0.7998222617196179}\n","{'loss': 0.509583984375, 'learning_rate': 9.177960453232615e-05, 'epoch': 0.822039546767385}\n","{'loss': 0.499595703125, 'learning_rate': 9.155743168184849e-05, 'epoch': 0.8442568318151522}\n","{'loss': 0.529822265625, 'learning_rate': 9.133525883137081e-05, 'epoch': 0.8664741168629193}\n","{'loss': 0.5348984375, 'learning_rate': 9.111308598089315e-05, 'epoch': 0.8886914019106865}\n","{'loss': 0.4939296875, 'learning_rate': 9.089091313041547e-05, 'epoch': 0.9109086869584537}\n","{'loss': 0.5190078125, 'learning_rate': 9.06687402799378e-05, 'epoch': 0.9331259720062208}\n","{'loss': 0.510501953125, 'learning_rate': 9.044656742946013e-05, 'epoch': 0.955343257053988}\n","{'loss': 0.474166015625, 'learning_rate': 9.022439457898245e-05, 'epoch': 0.9775605421017551}\n","{'loss': 0.47438671875, 'learning_rate': 9.000222172850478e-05, 'epoch': 0.9997778271495223}\n","{'loss': 0.502703125, 'learning_rate': 8.978004887802711e-05, 'epoch': 1.0219951121972894}\n","{'loss': 0.509568359375, 'learning_rate': 8.955787602754943e-05, 'epoch': 1.0442123972450568}\n","{'loss': 0.490587890625, 'learning_rate': 8.933570317707176e-05, 'epoch': 1.0664296822928239}\n","{'loss': 0.484658203125, 'learning_rate': 8.91135303265941e-05, 'epoch': 1.088646967340591}\n","{'loss': 0.504857421875, 'learning_rate': 8.889135747611642e-05, 'epoch': 1.110864252388358}\n","{'loss': 0.470615234375, 'learning_rate': 8.866918462563874e-05, 'epoch': 1.1330815374361254}\n","{'loss': 0.50691796875, 'learning_rate': 8.844701177516108e-05, 'epoch': 1.1552988224838925}\n","{'loss': 0.49902734375, 'learning_rate': 8.822483892468341e-05, 'epoch': 1.1775161075316596}\n","{'loss': 0.491375, 'learning_rate': 8.800266607420573e-05, 'epoch': 1.199733392579427}\n","{'loss': 0.522408203125, 'learning_rate': 8.778049322372807e-05, 'epoch': 1.221950677627194}\n","{'loss': 0.491375, 'learning_rate': 8.75583203732504e-05, 'epoch': 1.244167962674961}\n","{'loss': 0.50615625, 'learning_rate': 8.733614752277273e-05, 'epoch': 1.2663852477227282}\n","{'loss': 0.50991015625, 'learning_rate': 8.711397467229505e-05, 'epoch': 1.2886025327704955}\n","{'loss': 0.4714765625, 'learning_rate': 8.689180182181738e-05, 'epoch': 1.3108198178182626}\n","{'loss': 0.523185546875, 'learning_rate': 8.666962897133971e-05, 'epoch': 1.3330371028660297}\n","{'loss': 0.465240234375, 'learning_rate': 8.644745612086204e-05, 'epoch': 1.355254387913797}\n","{'loss': 0.517072265625, 'learning_rate': 8.622528327038436e-05, 'epoch': 1.3774716729615641}\n","{'loss': 0.486353515625, 'learning_rate': 8.60031104199067e-05, 'epoch': 1.3996889580093312}\n","{'loss': 0.4928671875, 'learning_rate': 8.578093756942902e-05, 'epoch': 1.4219062430570983}\n","{'loss': 0.5161796875, 'learning_rate': 8.555876471895134e-05, 'epoch': 1.4441235281048657}\n","{'loss': 0.50033203125, 'learning_rate': 8.533659186847368e-05, 'epoch': 1.4663408131526328}\n","{'loss': 0.48420703125, 'learning_rate': 8.5114419017996e-05, 'epoch': 1.4885580982003999}\n","{'loss': 0.4989609375, 'learning_rate': 8.489224616751834e-05, 'epoch': 1.5107753832481672}\n","{'loss': 0.45202734375, 'learning_rate': 8.467007331704067e-05, 'epoch': 1.5329926682959343}\n","{'loss': 0.49926171875, 'learning_rate': 8.4447900466563e-05, 'epoch': 1.5552099533437014}\n","{'loss': 0.49625, 'learning_rate': 8.422572761608532e-05, 'epoch': 1.5774272383914685}\n","{'loss': 0.49510546875, 'learning_rate': 8.400355476560765e-05, 'epoch': 1.5996445234392356}\n","{'loss': 0.49477734375, 'learning_rate': 8.378138191512998e-05, 'epoch': 1.621861808487003}\n","{'loss': 0.51975, 'learning_rate': 8.35592090646523e-05, 'epoch': 1.64407909353477}\n","{'loss': 0.486609375, 'learning_rate': 8.333703621417464e-05, 'epoch': 1.6662963785825373}\n","{'loss': 0.4838359375, 'learning_rate': 8.311486336369696e-05, 'epoch': 1.6885136636303044}\n","{'loss': 0.49262109375, 'learning_rate': 8.289269051321928e-05, 'epoch': 1.7107309486780715}\n","{'loss': 0.49757421875, 'learning_rate': 8.267051766274162e-05, 'epoch': 1.7329482337258386}\n","{'loss': 0.4896953125, 'learning_rate': 8.244834481226394e-05, 'epoch': 1.7551655187736057}\n","{'loss': 0.48730078125, 'learning_rate': 8.222617196178626e-05, 'epoch': 1.777382803821373}\n","{'loss': 0.52133984375, 'learning_rate': 8.20039991113086e-05, 'epoch': 1.7996000888691404}\n","{'loss': 0.492796875, 'learning_rate': 8.178182626083094e-05, 'epoch': 1.8218173739169075}\n","{'loss': 0.4854296875, 'learning_rate': 8.155965341035326e-05, 'epoch': 1.8440346589646746}\n","{'loss': 0.4840078125, 'learning_rate': 8.13374805598756e-05, 'epoch': 1.8662519440124417}\n","{'loss': 0.45432421875, 'learning_rate': 8.111530770939792e-05, 'epoch': 1.8884692290602088}\n","{'loss': 0.46915625, 'learning_rate': 8.089313485892024e-05, 'epoch': 1.9106865141079759}\n","{'loss': 0.48018359375, 'learning_rate': 8.067096200844258e-05, 'epoch': 1.9329037991557432}\n","{'loss': 0.4994921875, 'learning_rate': 8.04487891579649e-05, 'epoch': 1.9551210842035105}\n","{'loss': 0.4741796875, 'learning_rate': 8.022661630748722e-05, 'epoch': 1.9773383692512776}\n","{'loss': 0.482546875, 'learning_rate': 8.000444345700956e-05, 'epoch': 1.9995556542990447}\n","{'loss': 0.46769921875, 'learning_rate': 7.978227060653188e-05, 'epoch': 2.021772939346812}\n","{'loss': 0.48315625, 'learning_rate': 7.956009775605422e-05, 'epoch': 2.043990224394579}\n","{'loss': 0.49084765625, 'learning_rate': 7.933792490557654e-05, 'epoch': 2.066207509442346}\n","{'loss': 0.4975703125, 'learning_rate': 7.911575205509886e-05, 'epoch': 2.0884247944901135}\n","{'loss': 0.50215234375, 'learning_rate': 7.88935792046212e-05, 'epoch': 2.1106420795378806}\n","{'loss': 0.4315, 'learning_rate': 7.867140635414352e-05, 'epoch': 2.1328593645856477}\n","{'loss': 0.496234375, 'learning_rate': 7.844923350366586e-05, 'epoch': 2.155076649633415}\n","{'loss': 0.49616796875, 'learning_rate': 7.822706065318818e-05, 'epoch': 2.177293934681182}\n","{'loss': 0.49016796875, 'learning_rate': 7.800488780271052e-05, 'epoch': 2.199511219728949}\n","{'loss': 0.458703125, 'learning_rate': 7.778271495223284e-05, 'epoch': 2.221728504776716}\n","{'loss': 0.4690546875, 'learning_rate': 7.756054210175518e-05, 'epoch': 2.2439457898244832}\n","{'loss': 0.51022265625, 'learning_rate': 7.73383692512775e-05, 'epoch': 2.2661630748722508}\n","{'loss': 0.48315234375, 'learning_rate': 7.711619640079982e-05, 'epoch': 2.288380359920018}\n","{'loss': 0.46772265625, 'learning_rate': 7.689402355032216e-05, 'epoch': 2.310597644967785}\n","{'loss': 0.4678125, 'learning_rate': 7.667185069984448e-05, 'epoch': 2.332814930015552}\n","{'loss': 0.4660390625, 'learning_rate': 7.64496778493668e-05, 'epoch': 2.355032215063319}\n","{'loss': 0.47510546875, 'learning_rate': 7.622750499888914e-05, 'epoch': 2.3772495001110863}\n","{'loss': 0.49319921875, 'learning_rate': 7.600533214841146e-05, 'epoch': 2.399466785158854}\n","{'loss': 0.485671875, 'learning_rate': 7.578315929793379e-05, 'epoch': 2.421684070206621}\n","{'loss': 0.49919140625, 'learning_rate': 7.556098644745612e-05, 'epoch': 2.443901355254388}\n","{'loss': 0.47416015625, 'learning_rate': 7.533881359697845e-05, 'epoch': 2.466118640302155}\n","{'loss': 0.4721328125, 'learning_rate': 7.511664074650078e-05, 'epoch': 2.488335925349922}\n","{'loss': 0.4610234375, 'learning_rate': 7.489446789602312e-05, 'epoch': 2.5105532103976893}\n","{'loss': 0.50262890625, 'learning_rate': 7.467229504554544e-05, 'epoch': 2.5327704954454564}\n","{'loss': 0.49736328125, 'learning_rate': 7.445012219506777e-05, 'epoch': 2.5549877804932235}\n","{'loss': 0.4742109375, 'learning_rate': 7.42279493445901e-05, 'epoch': 2.577205065540991}\n","{'loss': 0.464171875, 'learning_rate': 7.400577649411242e-05, 'epoch': 2.599422350588758}\n","{'loss': 0.4946640625, 'learning_rate': 7.378360364363475e-05, 'epoch': 2.6216396356365252}\n","{'loss': 0.47209765625, 'learning_rate': 7.356143079315708e-05, 'epoch': 2.6438569206842923}\n","{'loss': 0.46168359375, 'learning_rate': 7.33392579426794e-05, 'epoch': 2.6660742057320594}\n","{'loss': 0.455578125, 'learning_rate': 7.311708509220173e-05, 'epoch': 2.6882914907798265}\n","{'loss': 0.49695703125, 'learning_rate': 7.289491224172407e-05, 'epoch': 2.710508775827594}\n","{'loss': 0.48143359375, 'learning_rate': 7.267273939124639e-05, 'epoch': 2.732726060875361}\n","{'loss': 0.48233984375, 'learning_rate': 7.245056654076871e-05, 'epoch': 2.7549433459231283}\n","{'loss': 0.4715390625, 'learning_rate': 7.222839369029105e-05, 'epoch': 2.7771606309708954}\n","{'loss': 0.48716796875, 'learning_rate': 7.200622083981337e-05, 'epoch': 2.7993779160186625}\n","{'loss': 0.45693359375, 'learning_rate': 7.17840479893357e-05, 'epoch': 2.8215952010664296}\n","{'loss': 0.4692578125, 'learning_rate': 7.156187513885804e-05, 'epoch': 2.8438124861141967}\n","{'loss': 0.47302734375, 'learning_rate': 7.133970228838037e-05, 'epoch': 2.866029771161964}\n","{'loss': 0.4440234375, 'learning_rate': 7.11175294379027e-05, 'epoch': 2.8882470562097313}\n","{'loss': 0.50518359375, 'learning_rate': 7.089535658742502e-05, 'epoch': 2.9104643412574984}\n","{'loss': 0.49421484375, 'learning_rate': 7.067318373694735e-05, 'epoch': 2.9326816263052655}\n","{'loss': 0.4875234375, 'learning_rate': 7.045101088646968e-05, 'epoch': 2.9548989113530326}\n","{'loss': 0.4802734375, 'learning_rate': 7.022883803599201e-05, 'epoch': 2.9771161964007997}\n","{'loss': 0.4927109375, 'learning_rate': 7.000666518551433e-05, 'epoch': 2.9993334814485673}\n","{'loss': 0.4824453125, 'learning_rate': 6.978449233503667e-05, 'epoch': 3.0215507664963344}\n","{'loss': 0.4778359375, 'learning_rate': 6.956231948455899e-05, 'epoch': 3.0437680515441015}\n","{'loss': 0.482359375, 'learning_rate': 6.934014663408131e-05, 'epoch': 3.0659853365918686}\n","{'loss': 0.4895390625, 'learning_rate': 6.911797378360365e-05, 'epoch': 3.0882026216396357}\n","{'loss': 0.4555546875, 'learning_rate': 6.889580093312597e-05, 'epoch': 3.1104199066874028}\n","{'loss': 0.4776015625, 'learning_rate': 6.86736280826483e-05, 'epoch': 3.13263719173517}\n","{'loss': 0.4517890625, 'learning_rate': 6.845145523217063e-05, 'epoch': 3.154854476782937}\n","{'loss': 0.4838046875, 'learning_rate': 6.822928238169297e-05, 'epoch': 3.177071761830704}\n","{'loss': 0.478125, 'learning_rate': 6.800710953121529e-05, 'epoch': 3.1992890468784716}\n","{'loss': 0.4739609375, 'learning_rate': 6.778493668073763e-05, 'epoch': 3.2215063319262387}\n","{'loss': 0.4813125, 'learning_rate': 6.756276383025995e-05, 'epoch': 3.243723616974006}\n","{'loss': 0.4721171875, 'learning_rate': 6.734059097978227e-05, 'epoch': 3.265940902021773}\n","{'loss': 0.4864921875, 'learning_rate': 6.711841812930461e-05, 'epoch': 3.28815818706954}\n","{'loss': 0.45646875, 'learning_rate': 6.689624527882693e-05, 'epoch': 3.310375472117307}\n","{'loss': 0.5001328125, 'learning_rate': 6.667407242834925e-05, 'epoch': 3.3325927571650746}\n","{'loss': 0.4536875, 'learning_rate': 6.645189957787159e-05, 'epoch': 3.3548100422128417}\n","{'loss': 0.47796875, 'learning_rate': 6.622972672739391e-05, 'epoch': 3.377027327260609}\n","{'loss': 0.475390625, 'learning_rate': 6.600755387691624e-05, 'epoch': 3.399244612308376}\n","{'loss': 0.496796875, 'learning_rate': 6.578538102643857e-05, 'epoch': 3.421461897356143}\n","{'loss': 0.461765625, 'learning_rate': 6.55632081759609e-05, 'epoch': 3.44367918240391}\n","{'loss': 0.4714765625, 'learning_rate': 6.534103532548323e-05, 'epoch': 3.4658964674516772}\n","{'loss': 0.453125, 'learning_rate': 6.511886247500555e-05, 'epoch': 3.4881137524994443}\n","{'loss': 0.46484375, 'learning_rate': 6.489668962452789e-05, 'epoch': 3.510331037547212}\n","{'loss': 0.46184375, 'learning_rate': 6.467451677405021e-05, 'epoch': 3.532548322594979}\n","{'loss': 0.4574140625, 'learning_rate': 6.445234392357255e-05, 'epoch': 3.554765607642746}\n","{'loss': 0.477015625, 'learning_rate': 6.423017107309487e-05, 'epoch': 3.576982892690513}\n","{'loss': 0.449765625, 'learning_rate': 6.40079982226172e-05, 'epoch': 3.5992001777382803}\n","{'loss': 0.48828125, 'learning_rate': 6.378582537213953e-05, 'epoch': 3.621417462786048}\n","{'loss': 0.475640625, 'learning_rate': 6.356365252166185e-05, 'epoch': 3.643634747833815}\n","{'loss': 0.4566328125, 'learning_rate': 6.334147967118419e-05, 'epoch': 3.665852032881582}\n","{'loss': 0.473453125, 'learning_rate': 6.311930682070651e-05, 'epoch': 3.688069317929349}\n","{'loss': 0.46528125, 'learning_rate': 6.289713397022884e-05, 'epoch': 3.710286602977116}\n","{'loss': 0.4736953125, 'learning_rate': 6.267496111975117e-05, 'epoch': 3.7325038880248833}\n","{'loss': 0.4633125, 'learning_rate': 6.24527882692735e-05, 'epoch': 3.7547211730726504}\n","{'loss': 0.48953125, 'learning_rate': 6.223061541879582e-05, 'epoch': 3.7769384581204175}\n","{'loss': 0.460015625, 'learning_rate': 6.200844256831815e-05, 'epoch': 3.7991557431681846}\n","{'loss': 0.473015625, 'learning_rate': 6.178626971784048e-05, 'epoch': 3.821373028215952}\n","{'loss': 0.4521640625, 'learning_rate': 6.156409686736281e-05, 'epoch': 3.8435903132637192}\n","{'loss': 0.4613359375, 'learning_rate': 6.134192401688515e-05, 'epoch': 3.8658075983114863}\n","{'loss': 0.4630859375, 'learning_rate': 6.111975116640747e-05, 'epoch': 3.8880248833592534}\n","{'loss': 0.45453125, 'learning_rate': 6.0897578315929795e-05, 'epoch': 3.9102421684070205}\n","{'loss': 0.484234375, 'learning_rate': 6.067540546545213e-05, 'epoch': 3.932459453454788}\n","{'loss': 0.4610234375, 'learning_rate': 6.0453232614974455e-05, 'epoch': 3.954676738502555}\n","{'loss': 0.4754296875, 'learning_rate': 6.023105976449678e-05, 'epoch': 3.9768940235503223}\n","{'loss': 0.48225, 'learning_rate': 6.0008886914019114e-05, 'epoch': 3.9991113085980894}\n","{'loss': 0.47334375, 'learning_rate': 5.9786714063541437e-05, 'epoch': 4.0213285936458565}\n","{'loss': 0.46590625, 'learning_rate': 5.956454121306376e-05, 'epoch': 4.043545878693624}\n","{'loss': 0.4775625, 'learning_rate': 5.9342368362586096e-05, 'epoch': 4.065763163741391}\n","{'loss': 0.4606875, 'learning_rate': 5.9120195512108425e-05, 'epoch': 4.087980448789158}\n","{'loss': 0.4722421875, 'learning_rate': 5.889802266163075e-05, 'epoch': 4.110197733836925}\n","{'loss': 0.4750390625, 'learning_rate': 5.8675849811153085e-05, 'epoch': 4.132415018884692}\n","{'loss': 0.457953125, 'learning_rate': 5.845367696067541e-05, 'epoch': 4.154632303932459}\n","{'loss': 0.460265625, 'learning_rate': 5.823150411019773e-05, 'epoch': 4.176849588980227}\n","{'loss': 0.4470546875, 'learning_rate': 5.8009331259720066e-05, 'epoch': 4.199066874027994}\n","{'loss': 0.463984375, 'learning_rate': 5.778715840924239e-05, 'epoch': 4.221284159075761}\n","{'loss': 0.4468828125, 'learning_rate': 5.756498555876472e-05, 'epoch': 4.243501444123528}\n","{'loss': 0.4888203125, 'learning_rate': 5.7342812708287055e-05, 'epoch': 4.2657187291712955}\n","{'loss': 0.4668515625, 'learning_rate': 5.712063985780938e-05, 'epoch': 4.287936014219063}\n","{'loss': 0.4555625, 'learning_rate': 5.68984670073317e-05, 'epoch': 4.31015329926683}\n","{'loss': 0.4459140625, 'learning_rate': 5.667629415685404e-05, 'epoch': 4.332370584314597}\n","{'loss': 0.452875, 'learning_rate': 5.645412130637636e-05, 'epoch': 4.354587869362364}\n","{'loss': 0.473703125, 'learning_rate': 5.623194845589869e-05, 'epoch': 4.376805154410131}\n","{'loss': 0.4664140625, 'learning_rate': 5.600977560542102e-05, 'epoch': 4.399022439457898}\n","{'loss': 0.46446875, 'learning_rate': 5.578760275494335e-05, 'epoch': 4.421239724505665}\n","{'loss': 0.4782734375, 'learning_rate': 5.5565429904465685e-05, 'epoch': 4.443457009553432}\n","{'loss': 0.45225, 'learning_rate': 5.534325705398801e-05, 'epoch': 4.465674294601199}\n","{'loss': 0.44340625, 'learning_rate': 5.512108420351033e-05, 'epoch': 4.4878915796489665}\n","{'loss': 0.4441328125, 'learning_rate': 5.489891135303267e-05, 'epoch': 4.510108864696734}\n","{'loss': 0.463625, 'learning_rate': 5.467673850255499e-05, 'epoch': 4.5323261497445015}\n","{'loss': 0.4486484375, 'learning_rate': 5.445456565207732e-05, 'epoch': 4.554543434792269}\n","{'loss': 0.44084375, 'learning_rate': 5.423239280159965e-05, 'epoch': 4.576760719840036}\n","{'loss': 0.475578125, 'learning_rate': 5.401021995112198e-05, 'epoch': 4.598978004887803}\n","{'loss': 0.4744609375, 'learning_rate': 5.37880471006443e-05, 'epoch': 4.62119528993557}\n","{'loss': 0.47040625, 'learning_rate': 5.356587425016664e-05, 'epoch': 4.643412574983337}\n","{'loss': 0.463078125, 'learning_rate': 5.334370139968896e-05, 'epoch': 4.665629860031104}\n","{'loss': 0.474375, 'learning_rate': 5.3121528549211284e-05, 'epoch': 4.687847145078871}\n","{'loss': 0.446453125, 'learning_rate': 5.289935569873362e-05, 'epoch': 4.710064430126638}\n","{'loss': 0.458984375, 'learning_rate': 5.267718284825594e-05, 'epoch': 4.732281715174405}\n","{'loss': 0.46559375, 'learning_rate': 5.245500999777827e-05, 'epoch': 4.7544990002221725}\n","{'loss': 0.4542734375, 'learning_rate': 5.223283714730061e-05, 'epoch': 4.77671628526994}\n","{'loss': 0.446453125, 'learning_rate': 5.201066429682293e-05, 'epoch': 4.798933570317708}\n","{'loss': 0.4334921875, 'learning_rate': 5.1788491446345254e-05, 'epoch': 4.821150855365475}\n","{'loss': 0.4683984375, 'learning_rate': 5.156631859586759e-05, 'epoch': 4.843368140413242}\n","{'loss': 0.4848046875, 'learning_rate': 5.1344145745389913e-05, 'epoch': 4.865585425461009}\n","{'loss': 0.4341875, 'learning_rate': 5.112197289491224e-05, 'epoch': 4.887802710508776}\n","{'loss': 0.467265625, 'learning_rate': 5.089980004443457e-05, 'epoch': 4.910019995556543}\n","{'loss': 0.480890625, 'learning_rate': 5.06776271939569e-05, 'epoch': 4.93223728060431}\n","{'loss': 0.4687578125, 'learning_rate': 5.0455454343479225e-05, 'epoch': 4.954454565652077}\n","{'loss': 0.4581796875, 'learning_rate': 5.023328149300156e-05, 'epoch': 4.976671850699844}\n","{'loss': 0.46465625, 'learning_rate': 5.0011108642523884e-05, 'epoch': 4.9988891357476115}\n","{'loss': 0.442671875, 'learning_rate': 4.9788935792046214e-05, 'epoch': 5.021106420795379}\n","{'loss': 0.4548125, 'learning_rate': 4.9566762941568543e-05, 'epoch': 5.043323705843146}\n","{'loss': 0.4443359375, 'learning_rate': 4.934459009109087e-05, 'epoch': 5.065540990890913}\n","{'loss': 0.4571640625, 'learning_rate': 4.91224172406132e-05, 'epoch': 5.08775827593868}\n","{'loss': 0.45715625, 'learning_rate': 4.890024439013553e-05, 'epoch': 5.109975560986447}\n","{'loss': 0.4659296875, 'learning_rate': 4.8678071539657855e-05, 'epoch': 5.132192846034215}\n","{'loss': 0.441328125, 'learning_rate': 4.8455898689180185e-05, 'epoch': 5.154410131081982}\n","{'loss': 0.4537578125, 'learning_rate': 4.8233725838702514e-05, 'epoch': 5.176627416129749}\n","{'loss': 0.4321484375, 'learning_rate': 4.801155298822484e-05, 'epoch': 5.198844701177516}\n","{'loss': 0.4606015625, 'learning_rate': 4.7789380137747167e-05, 'epoch': 5.221061986225283}\n","{'loss': 0.4449609375, 'learning_rate': 4.75672072872695e-05, 'epoch': 5.2432792712730505}\n","{'loss': 0.4535546875, 'learning_rate': 4.7345034436791826e-05, 'epoch': 5.265496556320818}\n","{'loss': 0.472359375, 'learning_rate': 4.7122861586314155e-05, 'epoch': 5.287713841368585}\n","{'loss': 0.4860546875, 'learning_rate': 4.6900688735836485e-05, 'epoch': 5.309931126416352}\n","{'loss': 0.4559140625, 'learning_rate': 4.667851588535881e-05, 'epoch': 5.332148411464119}\n","{'loss': 0.4333046875, 'learning_rate': 4.645634303488114e-05, 'epoch': 5.354365696511886}\n","{'loss': 0.460390625, 'learning_rate': 4.623417018440347e-05, 'epoch': 5.376582981559653}\n","{'loss': 0.4735390625, 'learning_rate': 4.6011997333925796e-05, 'epoch': 5.39880026660742}\n","{'loss': 0.4508046875, 'learning_rate': 4.5789824483448126e-05, 'epoch': 5.421017551655188}\n","{'loss': 0.456140625, 'learning_rate': 4.5567651632970456e-05, 'epoch': 5.443234836702955}\n","{'loss': 0.432734375, 'learning_rate': 4.5345478782492785e-05, 'epoch': 5.465452121750722}\n","{'loss': 0.4524140625, 'learning_rate': 4.512330593201511e-05, 'epoch': 5.4876694067984895}\n","{'loss': 0.456109375, 'learning_rate': 4.490113308153744e-05, 'epoch': 5.509886691846257}\n","{'loss': 0.45165625, 'learning_rate': 4.467896023105977e-05, 'epoch': 5.532103976894024}\n","{'loss': 0.4384296875, 'learning_rate': 4.445678738058209e-05, 'epoch': 5.554321261941791}\n","{'loss': 0.440703125, 'learning_rate': 4.4234614530104426e-05, 'epoch': 5.576538546989558}\n","{'loss': 0.46146875, 'learning_rate': 4.4012441679626756e-05, 'epoch': 5.598755832037325}\n","{'loss': 0.44640625, 'learning_rate': 4.379026882914908e-05, 'epoch': 5.620973117085092}\n","{'loss': 0.470640625, 'learning_rate': 4.356809597867141e-05, 'epoch': 5.643190402132859}\n","{'loss': 0.4638828125, 'learning_rate': 4.334592312819374e-05, 'epoch': 5.665407687180626}\n","{'loss': 0.45190625, 'learning_rate': 4.312375027771606e-05, 'epoch': 5.687624972228393}\n","{'loss': 0.45128125, 'learning_rate': 4.290157742723839e-05, 'epoch': 5.709842257276161}\n","{'loss': 0.4651171875, 'learning_rate': 4.267940457676072e-05, 'epoch': 5.732059542323928}\n","{'loss': 0.424734375, 'learning_rate': 4.245723172628305e-05, 'epoch': 5.7542768273716955}\n","{'loss': 0.4455625, 'learning_rate': 4.223505887580538e-05, 'epoch': 5.776494112419463}\n","{'loss': 0.480828125, 'learning_rate': 4.201288602532771e-05, 'epoch': 5.79871139746723}\n","{'loss': 0.4319921875, 'learning_rate': 4.179071317485004e-05, 'epoch': 5.820928682514997}\n","{'loss': 0.450171875, 'learning_rate': 4.156854032437236e-05, 'epoch': 5.843145967562764}\n","{'loss': 0.4370546875, 'learning_rate': 4.134636747389469e-05, 'epoch': 5.865363252610531}\n","{'loss': 0.445328125, 'learning_rate': 4.112419462341702e-05, 'epoch': 5.887580537658298}\n","{'loss': 0.4358671875, 'learning_rate': 4.090202177293935e-05, 'epoch': 5.909797822706065}\n","{'loss': 0.4545234375, 'learning_rate': 4.067984892246168e-05, 'epoch': 5.932015107753832}\n","{'loss': 0.4258046875, 'learning_rate': 4.045767607198401e-05, 'epoch': 5.954232392801599}\n","{'loss': 0.44578125, 'learning_rate': 4.023550322150633e-05, 'epoch': 5.9764496778493665}\n","{'loss': 0.436765625, 'learning_rate': 4.001333037102866e-05, 'epoch': 5.998666962897134}\n","{'loss': 0.43659375, 'learning_rate': 3.979115752055099e-05, 'epoch': 6.020884247944901}\n","{'loss': 0.4499140625, 'learning_rate': 3.9568984670073314e-05, 'epoch': 6.043101532992669}\n","{'loss': 0.4423125, 'learning_rate': 3.934681181959565e-05, 'epoch': 6.065318818040436}\n","{'loss': 0.444265625, 'learning_rate': 3.912463896911798e-05, 'epoch': 6.087536103088203}\n","{'loss': 0.45678125, 'learning_rate': 3.89024661186403e-05, 'epoch': 6.10975338813597}\n","{'loss': 0.44053125, 'learning_rate': 3.868029326816263e-05, 'epoch': 6.131970673183737}\n","{'loss': 0.441765625, 'learning_rate': 3.845812041768496e-05, 'epoch': 6.154187958231504}\n","{'loss': 0.469140625, 'learning_rate': 3.8235947567207285e-05, 'epoch': 6.176405243279271}\n","{'loss': 0.464546875, 'learning_rate': 3.8013774716729614e-05, 'epoch': 6.198622528327038}\n","{'loss': 0.454640625, 'learning_rate': 3.7791601866251944e-05, 'epoch': 6.2208398133748055}\n","{'loss': 0.43959375, 'learning_rate': 3.756942901577427e-05, 'epoch': 6.243057098422573}\n","{'loss': 0.42115625, 'learning_rate': 3.73472561652966e-05, 'epoch': 6.26527438347034}\n","{'loss': 0.445125, 'learning_rate': 3.712508331481893e-05, 'epoch': 6.287491668518107}\n","{'loss': 0.447828125, 'learning_rate': 3.690291046434126e-05, 'epoch': 6.309708953565874}\n","{'loss': 0.446734375, 'learning_rate': 3.6680737613863585e-05, 'epoch': 6.331926238613641}\n","{'loss': 0.4324375, 'learning_rate': 3.6458564763385915e-05, 'epoch': 6.354143523661408}\n","{'loss': 0.45425, 'learning_rate': 3.6236391912908244e-05, 'epoch': 6.376360808709176}\n","{'loss': 0.4451875, 'learning_rate': 3.6014219062430574e-05, 'epoch': 6.398578093756943}\n","{'loss': 0.465203125, 'learning_rate': 3.57920462119529e-05, 'epoch': 6.42079537880471}\n","{'loss': 0.43725, 'learning_rate': 3.556987336147523e-05, 'epoch': 6.443012663852477}\n","{'loss': 0.438859375, 'learning_rate': 3.5347700510997556e-05, 'epoch': 6.4652299489002445}\n","{'loss': 0.425953125, 'learning_rate': 3.5125527660519885e-05, 'epoch': 6.487447233948012}\n","{'loss': 0.447421875, 'learning_rate': 3.4903354810042215e-05, 'epoch': 6.509664518995779}\n","{'loss': 0.44846875, 'learning_rate': 3.468118195956454e-05, 'epoch': 6.531881804043546}\n","{'loss': 0.442796875, 'learning_rate': 3.445900910908687e-05, 'epoch': 6.554099089091313}\n","{'loss': 0.444234375, 'learning_rate': 3.4236836258609204e-05, 'epoch': 6.57631637413908}\n","{'loss': 0.455296875, 'learning_rate': 3.4014663408131526e-05, 'epoch': 6.598533659186847}\n","{'loss': 0.4469375, 'learning_rate': 3.3792490557653856e-05, 'epoch': 6.620750944234614}\n","{'loss': 0.465828125, 'learning_rate': 3.3570317707176186e-05, 'epoch': 6.642968229282381}\n","{'loss': 0.483421875, 'learning_rate': 3.3348144856698515e-05, 'epoch': 6.665185514330149}\n","{'loss': 0.4345, 'learning_rate': 3.312597200622084e-05, 'epoch': 6.687402799377916}\n","{'loss': 0.440921875, 'learning_rate': 3.290379915574317e-05, 'epoch': 6.7096200844256835}\n","{'loss': 0.43178125, 'learning_rate': 3.26816263052655e-05, 'epoch': 6.731837369473451}\n","{'loss': 0.459234375, 'learning_rate': 3.245945345478783e-05, 'epoch': 6.754054654521218}\n","{'loss': 0.44075, 'learning_rate': 3.2237280604310156e-05, 'epoch': 6.776271939568985}\n","{'loss': 0.430359375, 'learning_rate': 3.2015107753832486e-05, 'epoch': 6.798489224616752}\n","{'loss': 0.448296875, 'learning_rate': 3.179293490335481e-05, 'epoch': 6.820706509664519}\n","{'loss': 0.438875, 'learning_rate': 3.157076205287714e-05, 'epoch': 6.842923794712286}\n","{'loss': 0.45796875, 'learning_rate': 3.134858920239947e-05, 'epoch': 6.865141079760053}\n","{'loss': 0.4193125, 'learning_rate': 3.11264163519218e-05, 'epoch': 6.88735836480782}\n","{'loss': 0.434015625, 'learning_rate': 3.090424350144413e-05, 'epoch': 6.909575649855587}\n","{'loss': 0.43409375, 'learning_rate': 3.068207065096646e-05, 'epoch': 6.9317929349033545}\n","{'loss': 0.421421875, 'learning_rate': 3.045989780048878e-05, 'epoch': 6.9540102199511225}\n","{'loss': 0.4376875, 'learning_rate': 3.023772495001111e-05, 'epoch': 6.976227504998889}\n","{'loss': 0.439453125, 'learning_rate': 3.001555209953344e-05, 'epoch': 6.998444790046657}\n","{'loss': 0.4395625, 'learning_rate': 2.9793379249055768e-05, 'epoch': 7.020662075094424}\n","{'loss': 0.443859375, 'learning_rate': 2.9571206398578094e-05, 'epoch': 7.042879360142191}\n","{'loss': 0.428515625, 'learning_rate': 2.9349033548100424e-05, 'epoch': 7.065096645189958}\n","{'loss': 0.43225, 'learning_rate': 2.9126860697622754e-05, 'epoch': 7.087313930237725}\n","{'loss': 0.465578125, 'learning_rate': 2.890468784714508e-05, 'epoch': 7.109531215285492}\n","{'loss': 0.42884375, 'learning_rate': 2.868251499666741e-05, 'epoch': 7.131748500333259}\n","{'loss': 0.432796875, 'learning_rate': 2.846034214618974e-05, 'epoch': 7.153965785381026}\n","{'loss': 0.427046875, 'learning_rate': 2.8238169295712065e-05, 'epoch': 7.176183070428793}\n","{'loss': 0.44496875, 'learning_rate': 2.8015996445234395e-05, 'epoch': 7.1984003554765605}\n","{'loss': 0.4160625, 'learning_rate': 2.7793823594756724e-05, 'epoch': 7.220617640524328}\n","{'loss': 0.436296875, 'learning_rate': 2.7571650744279047e-05, 'epoch': 7.242834925572095}\n","{'loss': 0.461359375, 'learning_rate': 2.7349477893801377e-05, 'epoch': 7.265052210619862}\n","{'loss': 0.44825, 'learning_rate': 2.712730504332371e-05, 'epoch': 7.28726949566763}\n","{'loss': 0.44440625, 'learning_rate': 2.6905132192846033e-05, 'epoch': 7.309486780715397}\n","{'loss': 0.460546875, 'learning_rate': 2.6682959342368362e-05, 'epoch': 7.331704065763164}\n","{'loss': 0.4259375, 'learning_rate': 2.6460786491890692e-05, 'epoch': 7.353921350810931}\n","{'loss': 0.434453125, 'learning_rate': 2.6238613641413018e-05, 'epoch': 7.376138635858698}\n","{'loss': 0.450296875, 'learning_rate': 2.6016440790935348e-05, 'epoch': 7.398355920906465}\n","{'loss': 0.43503125, 'learning_rate': 2.5794267940457677e-05, 'epoch': 7.420573205954232}\n","{'loss': 0.402515625, 'learning_rate': 2.5572095089980007e-05, 'epoch': 7.4427904910019995}\n","{'loss': 0.42178125, 'learning_rate': 2.5349922239502333e-05, 'epoch': 7.465007776049767}\n","{'loss': 0.414828125, 'learning_rate': 2.5127749389024663e-05, 'epoch': 7.487225061097534}\n","{'loss': 0.45159375, 'learning_rate': 2.490557653854699e-05, 'epoch': 7.509442346145301}\n","{'loss': 0.4338125, 'learning_rate': 2.4683403688069322e-05, 'epoch': 7.531659631193068}\n","{'loss': 0.439203125, 'learning_rate': 2.4461230837591648e-05, 'epoch': 7.553876916240835}\n","{'loss': 0.45221875, 'learning_rate': 2.4239057987113974e-05, 'epoch': 7.576094201288603}\n","{'loss': 0.449390625, 'learning_rate': 2.4016885136636304e-05, 'epoch': 7.598311486336369}\n","{'loss': 0.4429375, 'learning_rate': 2.3794712286158633e-05, 'epoch': 7.620528771384137}\n","{'loss': 0.444484375, 'learning_rate': 2.357253943568096e-05, 'epoch': 7.642746056431904}\n","{'loss': 0.417640625, 'learning_rate': 2.335036658520329e-05, 'epoch': 7.664963341479671}\n","{'loss': 0.424453125, 'learning_rate': 2.312819373472562e-05, 'epoch': 7.6871806265274385}\n","{'loss': 0.43078125, 'learning_rate': 2.2906020884247948e-05, 'epoch': 7.709397911575206}\n","{'loss': 0.426578125, 'learning_rate': 2.2683848033770274e-05, 'epoch': 7.731615196622973}\n","{'loss': 0.41825, 'learning_rate': 2.24616751832926e-05, 'epoch': 7.75383248167074}\n","{'loss': 0.423546875, 'learning_rate': 2.2239502332814934e-05, 'epoch': 7.776049766718507}\n","{'loss': 0.468421875, 'learning_rate': 2.201732948233726e-05, 'epoch': 7.798267051766274}\n","{'loss': 0.419125, 'learning_rate': 2.1795156631859586e-05, 'epoch': 7.820484336814041}\n","{'loss': 0.42890625, 'learning_rate': 2.1572983781381916e-05, 'epoch': 7.842701621861808}\n","{'loss': 0.450234375, 'learning_rate': 2.1350810930904245e-05, 'epoch': 7.864918906909575}\n","{'loss': 0.418015625, 'learning_rate': 2.112863808042657e-05, 'epoch': 7.887136191957342}\n","{'loss': 0.421109375, 'learning_rate': 2.09064652299489e-05, 'epoch': 7.90935347700511}\n","{'loss': 0.449234375, 'learning_rate': 2.068429237947123e-05, 'epoch': 7.9315707620528775}\n","{'loss': 0.426234375, 'learning_rate': 2.046211952899356e-05, 'epoch': 7.953788047100645}\n","{'loss': 0.428625, 'learning_rate': 2.0239946678515886e-05, 'epoch': 7.976005332148412}\n","{'loss': 0.4555, 'learning_rate': 2.0017773828038213e-05, 'epoch': 7.998222617196179}\n","{'loss': 0.4235, 'learning_rate': 1.9795600977560546e-05, 'epoch': 8.020439902243945}\n","{'loss': 0.43546875, 'learning_rate': 1.9573428127082872e-05, 'epoch': 8.042657187291713}\n","{'loss': 0.445, 'learning_rate': 1.9351255276605198e-05, 'epoch': 8.064874472339481}\n","{'loss': 0.443265625, 'learning_rate': 1.9129082426127528e-05, 'epoch': 8.087091757387247}\n","{'loss': 0.432015625, 'learning_rate': 1.8906909575649857e-05, 'epoch': 8.109309042435015}\n","{'loss': 0.457515625, 'learning_rate': 1.8684736725172187e-05, 'epoch': 8.131526327482781}\n","{'loss': 0.422140625, 'learning_rate': 1.8462563874694513e-05, 'epoch': 8.15374361253055}\n","{'loss': 0.421609375, 'learning_rate': 1.824039102421684e-05, 'epoch': 8.175960897578316}\n","{'loss': 0.423578125, 'learning_rate': 1.8018218173739172e-05, 'epoch': 8.198178182626084}\n","{'loss': 0.42584375, 'learning_rate': 1.7796045323261498e-05, 'epoch': 8.22039546767385}\n","{'loss': 0.432328125, 'learning_rate': 1.7573872472783824e-05, 'epoch': 8.242612752721618}\n","{'loss': 0.4459375, 'learning_rate': 1.7351699622306154e-05, 'epoch': 8.264830037769384}\n","{'loss': 0.44034375, 'learning_rate': 1.7129526771828484e-05, 'epoch': 8.287047322817152}\n","{'loss': 0.401984375, 'learning_rate': 1.6907353921350813e-05, 'epoch': 8.309264607864918}\n","{'loss': 0.4291875, 'learning_rate': 1.668518107087314e-05, 'epoch': 8.331481892912686}\n","{'loss': 0.439359375, 'learning_rate': 1.646300822039547e-05, 'epoch': 8.353699177960454}\n","{'loss': 0.43409375, 'learning_rate': 1.62408353699178e-05, 'epoch': 8.37591646300822}\n","{'loss': 0.40325, 'learning_rate': 1.6018662519440125e-05, 'epoch': 8.398133748055988}\n","{'loss': 0.427390625, 'learning_rate': 1.579648966896245e-05, 'epoch': 8.420351033103755}\n","{'loss': 0.431390625, 'learning_rate': 1.5574316818484784e-05, 'epoch': 8.442568318151523}\n","{'loss': 0.4341875, 'learning_rate': 1.535214396800711e-05, 'epoch': 8.464785603199289}\n","{'loss': 0.438296875, 'learning_rate': 1.5129971117529438e-05, 'epoch': 8.487002888247057}\n","{'loss': 0.400265625, 'learning_rate': 1.4907798267051768e-05, 'epoch': 8.509220173294823}\n","{'loss': 0.405828125, 'learning_rate': 1.4685625416574096e-05, 'epoch': 8.531437458342591}\n","{'loss': 0.436015625, 'learning_rate': 1.4463452566096425e-05, 'epoch': 8.553654743390357}\n","{'loss': 0.408359375, 'learning_rate': 1.4241279715618751e-05, 'epoch': 8.575872028438125}\n","{'loss': 0.455703125, 'learning_rate': 1.401910686514108e-05, 'epoch': 8.598089313485891}\n","{'loss': 0.428359375, 'learning_rate': 1.3796934014663409e-05, 'epoch': 8.62030659853366}\n","{'loss': 0.416953125, 'learning_rate': 1.3574761164185737e-05, 'epoch': 8.642523883581426}\n","{'loss': 0.420109375, 'learning_rate': 1.3352588313708065e-05, 'epoch': 8.664741168629194}\n","{'loss': 0.420359375, 'learning_rate': 1.3130415463230394e-05, 'epoch': 8.686958453676962}\n","{'loss': 0.430578125, 'learning_rate': 1.2908242612752722e-05, 'epoch': 8.709175738724728}\n","{'loss': 0.415453125, 'learning_rate': 1.2686069762275052e-05, 'epoch': 8.731393023772496}\n","{'loss': 0.44253125, 'learning_rate': 1.246389691179738e-05, 'epoch': 8.753610308820262}\n","{'loss': 0.454453125, 'learning_rate': 1.2241724061319707e-05, 'epoch': 8.77582759386803}\n","{'loss': 0.40571875, 'learning_rate': 1.2019551210842035e-05, 'epoch': 8.798044878915796}\n","{'loss': 0.42771875, 'learning_rate': 1.1797378360364363e-05, 'epoch': 8.820262163963564}\n","{'loss': 0.447, 'learning_rate': 1.1575205509886693e-05, 'epoch': 8.84247944901133}\n","{'loss': 0.422, 'learning_rate': 1.135303265940902e-05, 'epoch': 8.864696734059098}\n","{'loss': 0.414171875, 'learning_rate': 1.1130859808931349e-05, 'epoch': 8.886914019106865}\n","{'loss': 0.412921875, 'learning_rate': 1.0908686958453678e-05, 'epoch': 8.909131304154633}\n","{'loss': 0.4344375, 'learning_rate': 1.0686514107976006e-05, 'epoch': 8.931348589202399}\n","{'loss': 0.39834375, 'learning_rate': 1.0464341257498336e-05, 'epoch': 8.953565874250167}\n","{'loss': 0.441140625, 'learning_rate': 1.0242168407020662e-05, 'epoch': 8.975783159297933}\n","{'loss': 0.43825, 'learning_rate': 1.0019995556542991e-05, 'epoch': 8.998000444345701}\n","{'loss': 0.413921875, 'learning_rate': 9.79782270606532e-06, 'epoch': 9.020217729393469}\n","{'loss': 0.41925, 'learning_rate': 9.575649855587647e-06, 'epoch': 9.042435014441235}\n","{'loss': 0.438375, 'learning_rate': 9.353477005109975e-06, 'epoch': 9.064652299489003}\n","{'loss': 0.418390625, 'learning_rate': 9.131304154632305e-06, 'epoch': 9.08686958453677}\n","{'loss': 0.42115625, 'learning_rate': 8.909131304154633e-06, 'epoch': 9.109086869584537}\n","{'loss': 0.448375, 'learning_rate': 8.68695845367696e-06, 'epoch': 9.131304154632303}\n","{'loss': 0.436, 'learning_rate': 8.464785603199288e-06, 'epoch': 9.153521439680071}\n","{'loss': 0.4160625, 'learning_rate': 8.242612752721618e-06, 'epoch': 9.175738724727838}\n","{'loss': 0.42053125, 'learning_rate': 8.020439902243946e-06, 'epoch': 9.197956009775606}\n","{'loss': 0.43621875, 'learning_rate': 7.798267051766274e-06, 'epoch': 9.220173294823372}\n","{'loss': 0.409953125, 'learning_rate': 7.5760942012886026e-06, 'epoch': 9.24239057987114}\n","{'loss': 0.44234375, 'learning_rate': 7.353921350810931e-06, 'epoch': 9.264607864918906}\n","{'loss': 0.39490625, 'learning_rate': 7.13174850033326e-06, 'epoch': 9.286825149966674}\n","{'loss': 0.40046875, 'learning_rate': 6.909575649855587e-06, 'epoch': 9.309042435014442}\n","{'loss': 0.4096875, 'learning_rate': 6.687402799377916e-06, 'epoch': 9.331259720062208}\n","{'loss': 0.419984375, 'learning_rate': 6.4652299489002446e-06, 'epoch': 9.353477005109976}\n","{'loss': 0.386953125, 'learning_rate': 6.243057098422573e-06, 'epoch': 9.375694290157742}\n","{'loss': 0.42184375, 'learning_rate': 6.020884247944902e-06, 'epoch': 9.39791157520551}\n","{'loss': 0.415125, 'learning_rate': 5.79871139746723e-06, 'epoch': 9.420128860253277}\n","{'loss': 0.39771875, 'learning_rate': 5.576538546989559e-06, 'epoch': 9.442346145301045}\n","{'loss': 0.406078125, 'learning_rate': 5.354365696511887e-06, 'epoch': 9.46456343034881}\n","{'loss': 0.432828125, 'learning_rate': 5.132192846034215e-06, 'epoch': 9.486780715396579}\n","{'loss': 0.397578125, 'learning_rate': 4.910019995556543e-06, 'epoch': 9.508998000444345}\n","{'loss': 0.3981875, 'learning_rate': 4.687847145078872e-06, 'epoch': 9.531215285492113}\n","{'loss': 0.434953125, 'learning_rate': 4.4656742946012e-06, 'epoch': 9.55343257053988}\n","{'loss': 0.411734375, 'learning_rate': 4.243501444123529e-06, 'epoch': 9.575649855587647}\n","{'loss': 0.437953125, 'learning_rate': 4.0213285936458565e-06, 'epoch': 9.597867140635415}\n","{'loss': 0.440984375, 'learning_rate': 3.799155743168185e-06, 'epoch': 9.620084425683181}\n","{'loss': 0.399109375, 'learning_rate': 3.5769828926905135e-06, 'epoch': 9.64230171073095}\n","{'loss': 0.4209375, 'learning_rate': 3.3548100422128414e-06, 'epoch': 9.664518995778716}\n","{'loss': 0.42434375, 'learning_rate': 3.13263719173517e-06, 'epoch': 9.686736280826484}\n","{'loss': 0.43828125, 'learning_rate': 2.9104643412574985e-06, 'epoch': 9.70895356587425}\n","{'loss': 0.4191875, 'learning_rate': 2.688291490779827e-06, 'epoch': 9.731170850922018}\n","{'loss': 0.371703125, 'learning_rate': 2.466118640302155e-06, 'epoch': 9.753388135969784}\n","{'loss': 0.410625, 'learning_rate': 2.243945789824484e-06, 'epoch': 9.775605421017552}\n","{'loss': 0.42409375, 'learning_rate': 2.021772939346812e-06, 'epoch': 9.797822706065318}\n","{'loss': 0.408125, 'learning_rate': 1.7996000888691405e-06, 'epoch': 9.820039991113086}\n","{'loss': 0.4309375, 'learning_rate': 1.5774272383914688e-06, 'epoch': 9.842257276160852}\n","{'loss': 0.411265625, 'learning_rate': 1.355254387913797e-06, 'epoch': 9.86447456120862}\n","{'loss': 0.417515625, 'learning_rate': 1.1330815374361255e-06, 'epoch': 9.886691846256387}\n","{'loss': 0.39353125, 'learning_rate': 9.109086869584538e-07, 'epoch': 9.908909131304155}\n","{'loss': 0.41828125, 'learning_rate': 6.887358364807821e-07, 'epoch': 9.931126416351923}\n","{'loss': 0.412375, 'learning_rate': 4.6656298600311046e-07, 'epoch': 9.953343701399689}\n","{'loss': 0.418140625, 'learning_rate': 2.4439013552543883e-07, 'epoch': 9.975560986447457}\n","{'loss': 0.409171875, 'learning_rate': 2.2217285047767164e-08, 'epoch': 9.997778271495223}\n","100% 225050/225050 [19:26:49<00:00,  3.21it/s]\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'epoch': 10.0}\n","100% 225050/225050 [19:26:49<00:00,  3.21it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/RCT_Fusion_chem_rct\n","Configuration saved in ../../../gdrive/MyDrive/RCT_Fusion_chem_rct/chemprot,rct-20k/adapter_fusion_config.json\n","Module weights saved in ../../../gdrive/MyDrive/RCT_Fusion_chem_rct/chemprot,rct-20k/pytorch_model_adapter_fusion.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/04/2020 22:48:46 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 30212\n","  Batch size = 8\n","100% 3777/3777 [07:24<00:00,  8.51it/s]\n","12/04/2020 22:56:10 - INFO - __main__ -   ***** Eval results adapter_fusion *****\n","12/04/2020 22:56:10 - INFO - __main__ -     eval_loss = 0.3803049623966217\n","12/04/2020 22:56:10 - INFO - __main__ -     eval_accuracy = 0.8686945584535946\n","12/04/2020 22:56:10 - INFO - __main__ -     eval_f1 = 0.8686945584535946\n","12/04/2020 22:56:10 - INFO - __main__ -     eval_precision = 0.8686945584535946\n","12/04/2020 22:56:10 - INFO - __main__ -     eval_recall = 0.8686945584535946\n","12/04/2020 22:56:10 - INFO - __main__ -     epoch = 10.0\n","12/04/2020 22:56:10 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 30135\n","  Batch size = 8\n","100% 3767/3767 [07:22<00:00,  8.51it/s]\n","12/04/2020 23:03:33 - INFO - __main__ -   ***** Test results adapter_fusion *****\n","12/04/2020 23:03:33 - INFO - __main__ -     eval_loss = 0.4145268201828003\n","12/04/2020 23:03:33 - INFO - __main__ -     eval_accuracy = 0.8624522979923677\n","12/04/2020 23:03:33 - INFO - __main__ -     eval_f1 = 0.8624522979923677\n","12/04/2020 23:03:33 - INFO - __main__ -     eval_precision = 0.8624522979923677\n","12/04/2020 23:03:33 - INFO - __main__ -     eval_recall = 0.8624522979923677\n","12/04/2020 23:03:33 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vETYFX5EAoYs","executionInfo":{"status":"ok","timestamp":1607140515057,"user_tz":300,"elapsed":4120,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"c8362b47-0d07-4eb0-f35c-0f2b68c46c3e"},"source":["%mkdir datasets\n","%cd datasets\n","%mkdir citation\n","%cd citation\n","!curl -Lo train.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/train.jsonl\n","!curl -Lo dev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/dev.jsonl\n","!curl -Lo test.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/test.jsonl\n","%cd ../.."],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons/datasets\n","/content/temp/adapticons/datasets/citation\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  454k  100  454k    0     0   289k      0  0:00:01  0:00:01 --:--:--  289k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 30138  100 30138    0     0  35498      0 --:--:-- --:--:-- --:--:-- 35456\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 38637  100 38637    0     0  46216      0 --:--:-- --:--:-- --:--:-- 46161\n","/content/temp/adapticons\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o9UEcYtwClVr","executionInfo":{"status":"ok","timestamp":1607140554091,"user_tz":300,"elapsed":310,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"23dafef1-4865-45c7-cf9c-afebd01727bf"},"source":["%cd modeling"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons/modeling\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-uYb8c92Cr3a","executionInfo":{"status":"ok","timestamp":1607140570904,"user_tz":300,"elapsed":524,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"5f6848c8-3f94-43c5-8c2c-9cf861e54ebc"},"source":["!ls ../datasets/citation"],"execution_count":13,"outputs":[{"output_type":"stream","text":["dev.jsonl  test.jsonl  train.jsonl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99PS9kUGC0Qi","executionInfo":{"status":"ok","timestamp":1607140609412,"user_tz":300,"elapsed":754,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"c9571788-0c4b-4f18-bfd1-4702f7f6d61b"},"source":["!ls ../../../gdrive/MyDrive/downsample_sciee/"],"execution_count":14,"outputs":[{"output_type":"stream","text":["_25_pct  _25_pct_adapter  _50_pct  _50_pct_adapter  _75_pct  _75_pct_adapter\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h07dQIlM-Oys","executionInfo":{"status":"ok","timestamp":1607141110373,"user_tz":300,"elapsed":435100,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"26f6d1dc-eed7-4d3f-acf0-3d6a80287ef5"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/citation/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter \\\n","  --task_name citation \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --train_adapter \\\n","  --model_name_or_path ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/ \\\n","  --adapter_config pfeiffer \\\n","  --metric macro \\\n","  --overwrite_output_dir "],"execution_count":15,"outputs":[{"output_type":"stream","text":["2020-12-05 03:57:57.981101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/05/2020 03:57:59 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/05/2020 03:57:59 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/downsample_newtrain/_25_adapter', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec05_03-57-59_68d9a55bde61', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/downsample_newtrain/_25_adapter', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_25_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"citation\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_25_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/vocab.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/ were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'citation_25_pct_adapter' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}}.\n","***** Running training *****\n","  Num examples = 1688\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1060\n","{'loss': 1.39570556640625, 'learning_rate': 1.0566037735849058e-05, 'epoch': 4.716981132075472}\n","{'loss': 1.219345703125, 'learning_rate': 1.1320754716981133e-06, 'epoch': 9.433962264150944}\n","100% 1060/1060 [06:41<00:00,  3.05it/s]\n","\n","Training completed. Do not forget to share your adapters on https://adapterhub.ml =)\n","\n","\n","{'epoch': 10.0}\n","100% 1060/1060 [06:41<00:00,  2.64it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter/citation_25_pct_adapter/adapter_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter/citation_25_pct_adapter/pytorch_adapter.bin\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter/citation_25_pct_adapter/head_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter/citation_25_pct_adapter/pytorch_model_head.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/05/2020 04:05:06 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 114\n","  Batch size = 8\n","100% 15/15 [00:01<00:00, 12.69it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","100% 15/15 [00:01<00:00, 12.94it/s]\n","12/05/2020 04:05:07 - INFO - __main__ -   ***** Eval results citation *****\n","12/05/2020 04:05:07 - INFO - __main__ -     eval_loss = 1.1132233142852783\n","12/05/2020 04:05:07 - INFO - __main__ -     eval_accuracy = 0.6228070175438597\n","12/05/2020 04:05:07 - INFO - __main__ -     eval_f1 = 0.23989937881821446\n","12/05/2020 04:05:07 - INFO - __main__ -     eval_precision = 0.21508563899868247\n","12/05/2020 04:05:07 - INFO - __main__ -     eval_recall = 0.2772397094430993\n","12/05/2020 04:05:07 - INFO - __main__ -     epoch = 10.0\n","12/05/2020 04:05:07 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 139\n","  Batch size = 8\n","100% 18/18 [00:01<00:00, 12.72it/s]\n","12/05/2020 04:05:09 - INFO - __main__ -   ***** Test results citation *****\n","12/05/2020 04:05:09 - INFO - __main__ -     eval_loss = 1.1687580347061157\n","12/05/2020 04:05:09 - INFO - __main__ -     eval_accuracy = 0.5827338129496403\n","12/05/2020 04:05:09 - INFO - __main__ -     eval_f1 = 0.2121280348230703\n","12/05/2020 04:05:09 - INFO - __main__ -     eval_precision = 0.19921980091471617\n","12/05/2020 04:05:09 - INFO - __main__ -     eval_recall = 0.24295774647887325\n","12/05/2020 04:05:09 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-06qidgKDI6a","executionInfo":{"status":"ok","timestamp":1607141631219,"user_tz":300,"elapsed":429783,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"e3aab1bd-fe82-4ae2-8647-572dc38d7636"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/citation/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter \\\n","  --task_name citation \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --train_adapter \\\n","  --model_name_or_path ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/ \\\n","  --adapter_config pfeiffer \\\n","  --metric macro \\\n","  --overwrite_output_dir "],"execution_count":16,"outputs":[{"output_type":"stream","text":["2020-12-05 04:06:43.668070: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/05/2020 04:06:45 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/05/2020 04:06:45 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/downsample_newtrain/_50_adapter', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec05_04-06-45_68d9a55bde61', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/downsample_newtrain/_50_adapter', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_50_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"citation\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_50_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/vocab.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/ were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'citation_50_pct_adapter' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}}.\n","***** Running training *****\n","  Num examples = 1688\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1060\n","{'loss': 1.3887174072265625, 'learning_rate': 1.0566037735849058e-05, 'epoch': 4.716981132075472}\n","{'loss': 1.2151182861328125, 'learning_rate': 1.1320754716981133e-06, 'epoch': 9.433962264150944}\n","100% 1060/1060 [06:41<00:00,  3.07it/s]\n","\n","Training completed. Do not forget to share your adapters on https://adapterhub.ml =)\n","\n","\n","{'epoch': 10.0}\n","100% 1060/1060 [06:41<00:00,  2.64it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter/citation_50_pct_adapter/adapter_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter/citation_50_pct_adapter/pytorch_adapter.bin\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter/citation_50_pct_adapter/head_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter/citation_50_pct_adapter/pytorch_model_head.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/05/2020 04:13:47 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 114\n","  Batch size = 8\n","100% 15/15 [00:01<00:00, 12.62it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","100% 15/15 [00:01<00:00, 12.93it/s]\n","12/05/2020 04:13:48 - INFO - __main__ -   ***** Eval results citation *****\n","12/05/2020 04:13:48 - INFO - __main__ -     eval_loss = 1.1108838319778442\n","12/05/2020 04:13:48 - INFO - __main__ -     eval_accuracy = 0.6052631578947368\n","12/05/2020 04:13:48 - INFO - __main__ -     eval_f1 = 0.22174398214570382\n","12/05/2020 04:13:48 - INFO - __main__ -     eval_precision = 0.20106382978723403\n","12/05/2020 04:13:48 - INFO - __main__ -     eval_recall = 0.25625504439063757\n","12/05/2020 04:13:48 - INFO - __main__ -     epoch = 10.0\n","12/05/2020 04:13:48 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 139\n","  Batch size = 8\n","100% 18/18 [00:01<00:00, 12.72it/s]\n","12/05/2020 04:13:49 - INFO - __main__ -   ***** Test results citation *****\n","12/05/2020 04:13:49 - INFO - __main__ -     eval_loss = 1.1648896932601929\n","12/05/2020 04:13:49 - INFO - __main__ -     eval_accuracy = 0.5899280575539568\n","12/05/2020 04:13:49 - INFO - __main__ -     eval_f1 = 0.21525553012967202\n","12/05/2020 04:13:49 - INFO - __main__ -     eval_precision = 0.20497198879551823\n","12/05/2020 04:13:49 - INFO - __main__ -     eval_recall = 0.24530516431924884\n","12/05/2020 04:13:49 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BIevYOxLDWOK","executionInfo":{"status":"ok","timestamp":1607142059630,"user_tz":300,"elapsed":858186,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"b973e44f-6cb8-420d-e737-2b6b3113d55e"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/citation/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter \\\n","  --task_name citation \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --train_adapter \\\n","  --model_name_or_path ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/ \\\n","  --adapter_config pfeiffer \\\n","  --metric macro \\\n","  --overwrite_output_dir "],"execution_count":17,"outputs":[{"output_type":"stream","text":["2020-12-05 04:13:52.468795: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/05/2020 04:13:54 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/05/2020 04:13:54 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/downsample_newtrain/_75_adapter', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec05_04-13-54_68d9a55bde61', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/downsample_newtrain/_75_adapter', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_75_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"citation\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_75_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/vocab.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/ were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'citation_75_pct_adapter' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}}.\n","***** Running training *****\n","  Num examples = 1688\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1060\n","{'loss': 1.38579541015625, 'learning_rate': 1.0566037735849058e-05, 'epoch': 4.716981132075472}\n","{'loss': 1.1944736328125, 'learning_rate': 1.1320754716981133e-06, 'epoch': 9.433962264150944}\n","100% 1060/1060 [06:41<00:00,  3.06it/s]\n","\n","Training completed. Do not forget to share your adapters on https://adapterhub.ml =)\n","\n","\n","{'epoch': 10.0}\n","100% 1060/1060 [06:41<00:00,  2.64it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter/citation_75_pct_adapter/adapter_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter/citation_75_pct_adapter/pytorch_adapter.bin\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter/citation_75_pct_adapter/head_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter/citation_75_pct_adapter/pytorch_model_head.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/05/2020 04:20:56 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 114\n","  Batch size = 8\n","100% 15/15 [00:01<00:00, 12.63it/s]/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","100% 15/15 [00:01<00:00, 12.91it/s]\n","12/05/2020 04:20:57 - INFO - __main__ -   ***** Eval results citation *****\n","12/05/2020 04:20:57 - INFO - __main__ -     eval_loss = 1.0685579776763916\n","12/05/2020 04:20:57 - INFO - __main__ -     eval_accuracy = 0.6140350877192983\n","12/05/2020 04:20:57 - INFO - __main__ -     eval_f1 = 0.2297374429223744\n","12/05/2020 04:20:57 - INFO - __main__ -     eval_precision = 0.1979565772669221\n","12/05/2020 04:20:57 - INFO - __main__ -     eval_recall = 0.2744148506860371\n","12/05/2020 04:20:57 - INFO - __main__ -     epoch = 10.0\n","12/05/2020 04:20:57 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 139\n","  Batch size = 8\n","100% 18/18 [00:01<00:00, 12.71it/s]\n","12/05/2020 04:20:58 - INFO - __main__ -   ***** Test results citation *****\n","12/05/2020 04:20:58 - INFO - __main__ -     eval_loss = 1.1221715211868286\n","12/05/2020 04:20:58 - INFO - __main__ -     eval_accuracy = 0.6115107913669064\n","12/05/2020 04:20:58 - INFO - __main__ -     eval_f1 = 0.22889948772301713\n","12/05/2020 04:20:58 - INFO - __main__ -     eval_precision = 0.2075438596491228\n","12/05/2020 04:20:58 - INFO - __main__ -     eval_recall = 0.2645359335500181\n","12/05/2020 04:20:58 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQGx7hdpJKJa","executionInfo":{"status":"ok","timestamp":1607142735464,"user_tz":300,"elapsed":421095,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"faf6a9c5-dee2-457a-e97e-36a1cb87a6a2"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/citation/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 1e-4 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter_2 \\\n","  --task_name citation \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --train_adapter \\\n","  --model_name_or_path ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/ \\\n","  --adapter_config pfeiffer \\\n","  --metric macro \\\n","  --overwrite_output_dir "],"execution_count":18,"outputs":[{"output_type":"stream","text":["2020-12-05 04:25:16.596318: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/05/2020 04:25:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/05/2020 04:25:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/downsample_newtrain/_25_adapter_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec05_04-25-18_68d9a55bde61', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/downsample_newtrain/_25_adapter_2', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_25_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"citation\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_25_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/vocab.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/ were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_25_pct_adapter/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'citation_25_pct_adapter' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}}.\n","***** Running training *****\n","  Num examples = 1688\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1060\n","{'loss': 1.0426612548828125, 'learning_rate': 5.283018867924528e-05, 'epoch': 4.716981132075472}\n","{'loss': 0.6267904052734375, 'learning_rate': 5.660377358490566e-06, 'epoch': 9.433962264150944}\n","100% 1060/1060 [06:41<00:00,  3.06it/s]\n","\n","Training completed. Do not forget to share your adapters on https://adapterhub.ml =)\n","\n","\n","{'epoch': 10.0}\n","100% 1060/1060 [06:41<00:00,  2.64it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter_2\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter_2/citation_25_pct_adapter/adapter_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter_2/citation_25_pct_adapter/pytorch_adapter.bin\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter_2/citation_25_pct_adapter/head_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_25_adapter_2/citation_25_pct_adapter/pytorch_model_head.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/05/2020 04:32:12 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 114\n","  Batch size = 8\n","100% 15/15 [00:01<00:00, 12.92it/s]\n","12/05/2020 04:32:13 - INFO - __main__ -   ***** Eval results citation *****\n","12/05/2020 04:32:13 - INFO - __main__ -     eval_loss = 0.754612386226654\n","12/05/2020 04:32:13 - INFO - __main__ -     eval_accuracy = 0.7017543859649122\n","12/05/2020 04:32:13 - INFO - __main__ -     eval_f1 = 0.5583873635370428\n","12/05/2020 04:32:13 - INFO - __main__ -     eval_precision = 0.6283796296296297\n","12/05/2020 04:32:13 - INFO - __main__ -     eval_recall = 0.5364339521119182\n","12/05/2020 04:32:13 - INFO - __main__ -     epoch = 10.0\n","12/05/2020 04:32:13 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 139\n","  Batch size = 8\n","100% 18/18 [00:01<00:00, 12.73it/s]\n","12/05/2020 04:32:14 - INFO - __main__ -   ***** Test results citation *****\n","12/05/2020 04:32:14 - INFO - __main__ -     eval_loss = 0.8713164925575256\n","12/05/2020 04:32:14 - INFO - __main__ -     eval_accuracy = 0.697841726618705\n","12/05/2020 04:32:14 - INFO - __main__ -     eval_f1 = 0.5461882094066001\n","12/05/2020 04:32:14 - INFO - __main__ -     eval_precision = 0.5668811668811669\n","12/05/2020 04:32:14 - INFO - __main__ -     eval_recall = 0.533628953206418\n","12/05/2020 04:32:14 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8YdUK0yJLPA","executionInfo":{"status":"ok","timestamp":1607143156283,"user_tz":300,"elapsed":841910,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"51ddd1f7-94a3-4b9a-8c6a-ab940583b01f"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/citation/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 1e-4 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter_2 \\\n","  --task_name citation \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --train_adapter \\\n","  --model_name_or_path ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/ \\\n","  --adapter_config pfeiffer \\\n","  --metric macro \\\n","  --overwrite_output_dir "],"execution_count":19,"outputs":[{"output_type":"stream","text":["2020-12-05 04:32:17.212872: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/05/2020 04:32:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/05/2020 04:32:19 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/downsample_newtrain/_50_adapter_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec05_04-32-19_68d9a55bde61', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/downsample_newtrain/_50_adapter_2', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_50_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"citation\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_50_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/vocab.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/ were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_50_pct_adapter/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'citation_50_pct_adapter' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}}.\n","***** Running training *****\n","  Num examples = 1688\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1060\n","{'loss': 1.0396195068359375, 'learning_rate': 5.283018867924528e-05, 'epoch': 4.716981132075472}\n","{'loss': 0.615138427734375, 'learning_rate': 5.660377358490566e-06, 'epoch': 9.433962264150944}\n","100% 1060/1060 [06:41<00:00,  3.06it/s]\n","\n","Training completed. Do not forget to share your adapters on https://adapterhub.ml =)\n","\n","\n","{'epoch': 10.0}\n","100% 1060/1060 [06:41<00:00,  2.64it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter_2\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter_2/citation_50_pct_adapter/adapter_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter_2/citation_50_pct_adapter/pytorch_adapter.bin\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter_2/citation_50_pct_adapter/head_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_50_adapter_2/citation_50_pct_adapter/pytorch_model_head.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/05/2020 04:39:12 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 114\n","  Batch size = 8\n","100% 15/15 [00:01<00:00, 12.95it/s]\n","12/05/2020 04:39:13 - INFO - __main__ -   ***** Eval results citation *****\n","12/05/2020 04:39:13 - INFO - __main__ -     eval_loss = 0.6917191743850708\n","12/05/2020 04:39:13 - INFO - __main__ -     eval_accuracy = 0.7631578947368421\n","12/05/2020 04:39:13 - INFO - __main__ -     eval_f1 = 0.6209274563820019\n","12/05/2020 04:39:13 - INFO - __main__ -     eval_precision = 0.6656470152020764\n","12/05/2020 04:39:13 - INFO - __main__ -     eval_recall = 0.6103847188592951\n","12/05/2020 04:39:13 - INFO - __main__ -     epoch = 10.0\n","12/05/2020 04:39:13 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 139\n","  Batch size = 8\n","100% 18/18 [00:01<00:00, 12.72it/s]\n","12/05/2020 04:39:15 - INFO - __main__ -   ***** Test results citation *****\n","12/05/2020 04:39:15 - INFO - __main__ -     eval_loss = 0.8512696027755737\n","12/05/2020 04:39:15 - INFO - __main__ -     eval_accuracy = 0.7266187050359713\n","12/05/2020 04:39:15 - INFO - __main__ -     eval_f1 = 0.5840627626341911\n","12/05/2020 04:39:15 - INFO - __main__ -     eval_precision = 0.5830240665766981\n","12/05/2020 04:39:15 - INFO - __main__ -     eval_recall = 0.6049904555538359\n","12/05/2020 04:39:15 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SHZdh6v2JL14","executionInfo":{"status":"ok","timestamp":1607143577285,"user_tz":300,"elapsed":1262908,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"8826fc7a-671e-4786-852a-ec2e9dc23f0f"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/citation/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 1e-4 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter_2 \\\n","  --task_name citation \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --train_adapter \\\n","  --model_name_or_path ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/ \\\n","  --adapter_config pfeiffer \\\n","  --metric macro \\\n","  --overwrite_output_dir "],"execution_count":20,"outputs":[{"output_type":"stream","text":["2020-12-05 04:39:18.040322: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/05/2020 04:39:19 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/05/2020 04:39:19 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/downsample_newtrain/_75_adapter_2', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec05_04-39-19_68d9a55bde61', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/downsample_newtrain/_75_adapter_2', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_75_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"citation\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"adapters\": {\n","    \"adapters\": {\n","      \"citation_75_pct_adapter\": [\n","        \"text_lang\",\n","        \"e12af4d77ed17a6b\"\n","      ]\n","    },\n","    \"config_map\": {\n","      \"e12af4d77ed17a6b\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      },\n","      \"text_lang\": {\n","        \"adapter_residual_before_ln\": false,\n","        \"invertible_adapter\": {\n","          \"block_type\": \"nice\",\n","          \"non_linearity\": \"relu\",\n","          \"reduction_factor\": 2\n","        },\n","        \"leave_out\": [],\n","        \"ln_after\": false,\n","        \"ln_before\": false,\n","        \"mh_adapter\": false,\n","        \"non_linearity\": \"relu\",\n","        \"original_ln_after\": true,\n","        \"original_ln_before\": true,\n","        \"output_adapter\": true,\n","        \"reduction_factor\": 12,\n","        \"residual_before_ln\": true\n","      }\n","    }\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/vocab.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/ were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_75_pct_adapter/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'citation_75_pct_adapter' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}}.\n","***** Running training *****\n","  Num examples = 1688\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1060\n","{'loss': 1.0334169921875, 'learning_rate': 5.283018867924528e-05, 'epoch': 4.716981132075472}\n","{'loss': 0.6104193115234375, 'learning_rate': 5.660377358490566e-06, 'epoch': 9.433962264150944}\n","100% 1060/1060 [06:41<00:00,  3.06it/s]\n","\n","Training completed. Do not forget to share your adapters on https://adapterhub.ml =)\n","\n","\n","{'epoch': 10.0}\n","100% 1060/1060 [06:41<00:00,  2.64it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter_2\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter_2/citation_75_pct_adapter/adapter_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter_2/citation_75_pct_adapter/pytorch_adapter.bin\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter_2/citation_75_pct_adapter/head_config.json\n","Module weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_75_adapter_2/citation_75_pct_adapter/pytorch_model_head.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/05/2020 04:46:13 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 114\n","  Batch size = 8\n","100% 15/15 [00:01<00:00, 12.94it/s]\n","12/05/2020 04:46:14 - INFO - __main__ -   ***** Eval results citation *****\n","12/05/2020 04:46:14 - INFO - __main__ -     eval_loss = 0.6905847787857056\n","12/05/2020 04:46:14 - INFO - __main__ -     eval_accuracy = 0.7631578947368421\n","12/05/2020 04:46:14 - INFO - __main__ -     eval_f1 = 0.6263772175536881\n","12/05/2020 04:46:14 - INFO - __main__ -     eval_precision = 0.670327168277988\n","12/05/2020 04:46:14 - INFO - __main__ -     eval_recall = 0.615496368038741\n","12/05/2020 04:46:14 - INFO - __main__ -     epoch = 10.0\n","12/05/2020 04:46:14 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 139\n","  Batch size = 8\n","100% 18/18 [00:01<00:00, 12.73it/s]\n","12/05/2020 04:46:16 - INFO - __main__ -   ***** Test results citation *****\n","12/05/2020 04:46:16 - INFO - __main__ -     eval_loss = 0.8714953064918518\n","12/05/2020 04:46:16 - INFO - __main__ -     eval_accuracy = 0.7410071942446043\n","12/05/2020 04:46:16 - INFO - __main__ -     eval_f1 = 0.6055918015271141\n","12/05/2020 04:46:16 - INFO - __main__ -     eval_precision = 0.6177486094152761\n","12/05/2020 04:46:16 - INFO - __main__ -     eval_recall = 0.609685291234587\n","12/05/2020 04:46:16 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]}]}