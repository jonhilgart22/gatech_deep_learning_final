{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check for train.py in don't stop pre-training repo\n",
    "- This notebook tests functionality of train.py in don't stop pre-training repo. \n",
    "It checks : \n",
    "    - Whether the script (train.py) loads and trains relevant model\n",
    "    - Whether it performs fine-tuning on the weights of adapters (while freezing the other params from Roberta model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load pre-trained model (adapter + roberta) and fine-tuned model\n",
    "- Here model_path is the path of your pre-trained adapter model (the name of checkpoint would be pytorch_model.bin)\n",
    "- model_path2 is the path of your fine-tuned adapter model by executing train.py in don't stop repo (the name of checkpoint would be best.th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model with weights of pre-trained model (pre-trained adapter + Roberta)\n",
    "\n",
    "model_path = './tapt/roberta-tapt-help-adapter/checkpoint-45000/pytorch_model.bin'### Here plug in pretrained adapter model checkpoint path\n",
    "model_adapter = torch.load(model_path)\n",
    "\n",
    "## load model with weights of fine-tuned model (model after fine-tuning pre-trained (adapter + Roberta))\n",
    "\n",
    "model_path2 = './model_logs/amazon_tapt_adapter_33epochs/best.th' ### Here plug in fine-tuned pre-trained (adapter + Roberta) model checkpoint path\n",
    "model_ft = torch.load(model_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Print the name of each layer\n",
    "- Print the layers with its name for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## printing keys for each model\n",
    "## pre-trained model\n",
    "for key in model_adapter:\n",
    "    print('keys of pre-trained: ', key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fine-tuned model\n",
    "for key2 in model_ft:\n",
    "    print('keys of fine-tuned: ', key2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test 1\n",
    "diff_sum = 0\n",
    "test_3 = False\n",
    "for keys in model_adapter:\n",
    "    if 'adapter' in keys:\n",
    "        assert keys in model_ft, 'There is no adapter layer in the fine-tuned model!'\n",
    "        if keys in model_ft:\n",
    "            diff_sum += torch.abs(torch.sum(model_ft[keys] - model_adapter[keys])).cpu().numpy()\n",
    "    else:\n",
    "        continue\n",
    "print('Test 1 passed!')        \n",
    "test_3 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test 2\n",
    "adapter_layer_count = 0\n",
    "test_3_2 = False\n",
    "for keys in model_ft:\n",
    "    if 'adapter' in keys:\n",
    "        adapter_layer_count += 1\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "assert adapter_layer_count > 0, 'There is no adapter layer in the fine-tuned model! adapter layer count is ' + str(adapter_layer_count)\n",
    "print('Test 2 passed!')\n",
    "test_3_2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test 3\n",
    "if test_3 and test_3_2:\n",
    "    assert diff_sum != 0, \"The weights of adpater are frozen!\"\n",
    "    print('Test 3 passed!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:domains] *",
   "language": "python",
   "name": "conda-env-domains-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
