{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/jonathanhilgart/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/jonathanhilgart/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /Users/jonathanhilgart/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# output more information\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# load pre-trained BERT tokenizer from Huggingface\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenize an input sentence\n",
    "sentence = \"It's also, clearly, great fun.\"\n",
    "sentence_negative = \"this is really the worst day in history. it sucks\"\n",
    "\n",
    "# convert input tokens to indices and create PyTorch input tensor\n",
    "input_tensor = torch.tensor([tokenizer.encode(sentence)])\n",
    "\n",
    "input_tensor_negative = torch.tensor([tokenizer.encode(sentence_negative)])\n",
    "\n",
    "# load pre-trained BERT model from Huggingface\n",
    "# the `BertForSequenceClassification` class includes a prediction head for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2009, 1005, 1055, 2036, 1010, 4415, 1010, 2307, 4569, 1012,  102]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now add a pre-trained task adapter that is useful to our task from Adapter Hub.\n",
    "- As weâ€™re doing sentiment classification, we use an adapter trained on the SST-2 dataset in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.adapter_utils:Found matching adapter at: adapters/ukp/bert-base-uncased_sentiment_sst-2_pfeiffer.json\n",
      "INFO:transformers.adapter_utils:Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_task/sst/bert-base-uncased/pfeiffer/bert-base-uncased_sentiment_sst-2_pfeiffer.zip.\n",
      "INFO:transformers.adapter_model_mixin:Loading module configuration from /Users/jonathanhilgart/.cache/torch/adapters/f23c9704bc526e1a5c605a1f1c76e7225da0fff90086a7e9483da11de926d624-04066537e8abe7c5ee72d7804a94afca7d5ff566b6731d82c77951cc3493ed8a-extracted/adapter_config.json\n",
      "INFO:transformers.adapter_config:Adding adapter 'sst-2' of type 'text_task'.\n",
      "INFO:transformers.adapter_model_mixin:Loading module weights from /Users/jonathanhilgart/.cache/torch/adapters/f23c9704bc526e1a5c605a1f1c76e7225da0fff90086a7e9483da11de926d624-04066537e8abe7c5ee72d7804a94afca7d5ff566b6731d82c77951cc3493ed8a-extracted/pytorch_adapter.bin\n",
      "INFO:transformers.adapter_model_mixin:Loading module configuration from /Users/jonathanhilgart/.cache/torch/adapters/f23c9704bc526e1a5c605a1f1c76e7225da0fff90086a7e9483da11de926d624-04066537e8abe7c5ee72d7804a94afca7d5ff566b6731d82c77951cc3493ed8a-extracted/head_config.json\n",
      "INFO:transformers.adapter_model_mixin:Loading module weights from /Users/jonathanhilgart/.cache/torch/adapters/f23c9704bc526e1a5c605a1f1c76e7225da0fff90086a7e9483da11de926d624-04066537e8abe7c5ee72d7804a94afca7d5ff566b6731d82c77951cc3493ed8a-extracted/pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained task adapter from Adapter Hub\n",
    "# this method call will also load a pre-trained classification head for the adapter task\n",
    "adapter_name = model.load_adapter('sst-2@ukp', config='pfeiffer')\n",
    "\n",
    "# activate the adapter we just loaded, so that it is used in every forward pass\n",
    "model.set_active_adapters(adapter_name)\n",
    "\n",
    "# predict output tensor\n",
    "outputs = model(input_tensor)\n",
    "ouputs_negative_sentiment = model(input_tensor_negative)\n",
    "\n",
    "# retrieve the predicted class label\n",
    "predicted = torch.argmax(outputs[0]).item()\n",
    "assert predicted == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-4.2206,  3.9714]], grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.8247, -3.5260]], grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ouputs_negative_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(ouputs_negative_sentiment[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapter Types\n",
    "- Task adapter: Task adapters are fine-tuned to learn representations for a specific downstream tasks such as sentiment analysis, question answering etc. Task adapters for NLP were first introduced by Houlsby et al., 2019.\n",
    "\n",
    "- Language adapter: Language adapters are used to learn language-specific transformations. After being trained on a language modeling task, a language adapter can be stacked before a task adapter for training on a downstream task. To perform zero-shot cross-lingual transfer, one language adapter can simply be replaced by another. In terms of architecture, language adapters are largely similar to task adapters, except for an additional invertible adapter layer after the embedding layer. This setup was introduced and is further explained by Pfeiffer et al., 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapticons",
   "language": "python",
   "name": "adapticons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
