{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_Project_06.ipynb","provenance":[],"authorship_tag":"ABX9TyMfbTDHlZXr6vTcq0haruxO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"j2wIUNYcHyRe"},"source":["!pip install git+https://github.com/kernelmachine/allennlp.git@4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7\n","!pip install transformers==2.6.0\n","!pip install pytorch-transformers==1.2.0\n","!git clone https://github.com/allenai/dont-stop-pretraining"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQTM68LbH3JK"},"source":["%cd /content/dont-stop-pretraining"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Tj5mDBlH8qh"},"source":["import torch\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(device)\n","\n","!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"evLB3l1nH3jZ"},"source":["!curl -Lo cittrain.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/train.jsonl\n","!curl -Lo citdev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/dev.jsonl\n","!curl -Lo citest.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/test.jsonl\n","\n","!curl -Lo hyptrain.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/hyperpartisan_news/train.jsonl\n","!curl -Lo hypdev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/hyperpartisan_news/dev.jsonl\n","!curl -Lo hyptest.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/hyperpartisan_news/test.jsonl\n","\n","!curl -Lo sciietrain.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/sciie/train.jsonl\n","!curl -Lo sciiedev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/sciie/dev.jsonl\n","!curl -Lo sciieest.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/sciie/test.jsonl\n","\n","import pandas as pd\n","import json\n","data = []\n","with open(\"hyptrain.jsonl\", 'r', encoding='utf-8') as f:\n","    for line in f:\n","       data.append(json.loads(line))\n","df = pd.DataFrame(data)\n","df.text.to_csv(\"hyptrain.txt\", header=False, index=False)\n","data = []\n","with open(\"hypdev.jsonl\", 'r', encoding='utf-8') as f:\n","    for line in f:\n","       data.append(json.loads(line))\n","df = pd.DataFrame(data)\n","df.text.to_csv(\"hypdev.txt\", header=False, index=False)\n","\n","\n","data = []\n","with open(\"cittrain.jsonl\", 'r', encoding='utf-8') as f:\n","    for line in f:\n","       data.append(json.loads(line))\n","df = pd.DataFrame(data)\n","df.text.to_csv(\"cittrain.txt\", header=False, index=False)\n","data = []\n","with open(\"citdev.jsonl\", 'r', encoding='utf-8') as f:\n","    for line in f:\n","       data.append(json.loads(line))\n","df = pd.DataFrame(data)\n","df.text.to_csv(\"citdev.txt\", header=False, index=False)\n","\n","data = []\n","with open(\"sciietrain.jsonl\", 'r', encoding='utf-8') as f:\n","    for line in f:\n","       data.append(json.loads(line))\n","df = pd.DataFrame(data)\n","df.text.to_csv(\"sciietrain.txt\", header=False, index=False)\n","data = []\n","with open(\"sciiedev.jsonl\", 'r', encoding='utf-8') as f:\n","    for line in f:\n","       data.append(json.loads(line))\n","df = pd.DataFrame(data)\n","df.text.to_csv(\"sciiedev.txt\", header=False, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tLD91EUxIZQh"},"source":["!python -m scripts.run_language_modeling \\\n","                --train_data_file hyptrain.txt \\\n","                --line_by_line \\\n","                --output_dir roberta_tapt3 \\\n","                --model_type roberta-base \\\n","                --tokenizer_name roberta-base \\\n","                --mlm \\\n","                --per_gpu_train_batch_size 8 \\\n","                --gradient_accumulation_steps 16  \\\n","                --model_name_or_path roberta-base \\\n","                --eval_data_file hypdev.txt \\\n","                --do_eval \\\n","                --evaluate_during_training  \\\n","                --do_train \\\n","                --num_train_epochs 100  \\\n","                --learning_rate 0.0001 \\\n","                --logging_steps 50"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pQb33g2cIw55"},"source":["!python -m scripts.run_language_modeling \\\n","                --train_data_file cittrain.txt \\\n","                --line_by_line \\\n","                --output_dir roberta_tapt_cit_01 \\\n","                --model_type roberta-base \\\n","                --tokenizer_name roberta-base \\\n","                --mlm \\\n","                --per_gpu_train_batch_size 8 \\\n","                --gradient_accumulation_steps 16  \\\n","                --model_name_or_path roberta-base \\\n","                --eval_data_file citdev.txt \\\n","                --do_eval \\\n","                --evaluate_during_training  \\\n","                --do_train \\\n","                --num_train_epochs 100  \\\n","                --learning_rate 0.0001 \\\n","                --logging_steps 50"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37sGxB2nI8E5"},"source":["!python -m scripts.run_language_modeling \\\n","                --train_data_file sciietrain.txt \\\n","                --line_by_line \\\n","                --output_dir roberta_tapt_sciie_01 \\\n","                --model_type roberta-base \\\n","                --tokenizer_name roberta-base \\\n","                --mlm \\\n","                --per_gpu_train_batch_size 8 \\\n","                --gradient_accumulation_steps 16  \\\n","                --model_name_or_path roberta-base \\\n","                --eval_data_file sciiedev.txt \\\n","                --do_eval \\\n","                --evaluate_during_training  \\\n","                --do_train \\\n","                --num_train_epochs 100  \\\n","                --learning_rate 0.0001 \\\n","                --logging_steps 50"],"execution_count":null,"outputs":[]}]}