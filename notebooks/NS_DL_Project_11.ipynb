{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_Project_11.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMzdgnEErlYCu/AM6AzU1CZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bE3OQ8F3giBO","executionInfo":{"status":"ok","timestamp":1607135096038,"user_tz":300,"elapsed":2891,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"3cd60d20-54ec-4ccf-dc9d-a60e7c35ecef"},"source":["import torch\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(device)\n","\n","!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["cuda\n","Sat Dec  5 02:24:56 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    24W / 300W |     10MiB / 16130MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NnAq9TaQnkPe","executionInfo":{"status":"ok","timestamp":1607135238191,"user_tz":300,"elapsed":139291,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"8c786d2f-439e-492a-d44f-4843d9759c0e"},"source":["!pip install git+https://github.com/kernelmachine/allennlp.git@4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7\n","!pip install transformers==2.4.1\n","!pip install pytorch-transformers==1.2.0\n","!git clone https://github.com/allenai/dont-stop-pretraining\n","\n","!pip install adapter-transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/kernelmachine/allennlp.git@4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7\n","  Cloning https://github.com/kernelmachine/allennlp.git (to revision 4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7) to /tmp/pip-req-build-f85acw74\n","  Running command git clone -q https://github.com/kernelmachine/allennlp.git /tmp/pip-req-build-f85acw74\n","  Running command git checkout -q 4ae123d2c3bfb1ea3ce7362cb6c5bca3d094ffa7\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n","\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n","\u001b[?25hCollecting conllu==1.3.1\n","  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n","Collecting pytorch-pretrained-bert>=0.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 17.2MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (2018.9)\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/9c/544396572c05841b7a2482c88be5dd54dcd18ba97abeb1e8d34daf921a54/boto3-1.16.30-py2.py3-none-any.whl (129kB)\n","\u001b[K     |████████████████████████████████| 133kB 20.0MB/s \n","\u001b[?25hCollecting word2number>=1.1\n","  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (3.6.4)\n","Collecting parsimonious>=0.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (3.2.2)\n","Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (1.7.0+cu101)\n","Collecting jsonpickle\n","  Downloading https://files.pythonhosted.org/packages/ee/d5/1cc282dc23346a43aab461bf2e8c36593aacd34242bee1a13fa750db0cfe/jsonpickle-1.4.2-py2.py3-none-any.whl\n","Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (2.23.0)\n","Collecting flask-cors>=3.0.7\n","  Downloading https://files.pythonhosted.org/packages/69/7f/d0aeaaafb5c3c76c8d2141dbe2d4f6dca5d6c31872d4e5349768c1958abc/Flask_Cors-3.0.9-py2.py3-none-any.whl\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (2.10.0)\n","Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (0.5.3)\n","Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (0.4.1)\n","Collecting numpydoc>=0.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n","\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n","\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n","\u001b[K     |████████████████████████████████| 266kB 20.6MB/s \n","\u001b[?25hCollecting flaky\n","  Downloading https://files.pythonhosted.org/packages/43/0e/2f50064e327f41a1eb811df089f813036e19a64b95e33f8e9e0b96c2447e/flaky-3.7.0-py2.py3-none-any.whl\n","Collecting overrides\n","  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (0.22.2.post1)\n","Collecting tensorboardX>=1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\u001b[K     |████████████████████████████████| 317kB 24.2MB/s \n","\u001b[?25hCollecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\u001b[K     |████████████████████████████████| 245kB 32.9MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (3.2.5)\n","Collecting gevent>=1.3.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/92/b80b922f08f222faca53c8d278e2e612192bc74b0e1f0db2f80a6ee46982/gevent-20.9.0-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n","\u001b[K     |████████████████████████████████| 5.3MB 24.8MB/s \n","\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (1.1.2)\n","Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (4.41.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (1.4.1)\n","Collecting responses>=0.7\n","  Downloading https://files.pythonhosted.org/packages/d5/71/4f04aed03ca35f2d02e1732ca6e996b2d7b40232fb7f1b58ff35f9a89b7b/responses-0.12.1-py2.py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp===0.9.1-unreleased) (1.18.5)\n","Collecting spacy<2.2,>=2.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n","\u001b[K     |████████████████████████████████| 30.9MB 180kB/s \n","\u001b[?25hCollecting pytorch-transformers==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n","\u001b[K     |████████████████████████████████| 163kB 64.6MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp===0.9.1-unreleased) (0.2.5)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp===0.9.1-unreleased) (2019.12.20)\n","Collecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 9.8MB/s \n","\u001b[?25hCollecting botocore<1.20.0,>=1.19.30\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/a3/1ee497faf994d180df5d14d456eef1ef46ca1ffce617816faa4ff8164608/botocore-1.19.30-py2.py3-none-any.whl (7.0MB)\n","\u001b[K     |████████████████████████████████| 7.0MB 59.6MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (20.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (1.15.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (8.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (50.3.2)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (1.4.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (0.7.1)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp===0.9.1-unreleased) (1.9.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp===0.9.1-unreleased) (1.3.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp===0.9.1-unreleased) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp===0.9.1-unreleased) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp===0.9.1-unreleased) (0.8)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp===0.9.1-unreleased) (2.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp===0.9.1-unreleased) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp===0.9.1-unreleased) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp===0.9.1-unreleased) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp===0.9.1-unreleased) (3.0.4)\n","Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.8.5)\n","Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.11.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp===0.9.1-unreleased) (0.17.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp===0.9.1-unreleased) (3.12.4)\n","Collecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d0/532e160c777b42f6f393f9de8c88abb8af6c892037c55e4d3a8a211324dd/greenlet-0.4.17-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n","\u001b[?25hCollecting zope.event\n","  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n","Collecting zope.interface\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/b0/da8afd9b3bd50c7665ecdac062f182982af1173c9081f9af7261091c5588/zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n","\u001b[K     |████████████████████████████████| 245kB 55.0MB/s \n","\u001b[?25hRequirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp===0.9.1-unreleased) (7.1.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp===0.9.1-unreleased) (1.1.0)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp===0.9.1-unreleased) (1.0.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (0.8.0)\n","Collecting preshed<2.1.0,>=2.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n","\u001b[K     |████████████████████████████████| 92kB 12.4MB/s \n","\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (2.0.4)\n","Collecting plac<1.0.0,>=0.9.6\n","  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n","Collecting thinc<7.1.0,>=7.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 55.8MB/s \n","\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (1.0.4)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp===0.9.1-unreleased) (1.0.4)\n","Collecting blis<0.3.0,>=0.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 48.5MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 43.9MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp===0.9.1-unreleased) (3.4.0)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.2.0)\n","Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.9.0)\n","Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (0.16)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.0.0)\n","Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.2.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (20.4)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (2.6.1)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (0.7.12)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.1.1)\n","Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp===0.9.1-unreleased) (1.1.4)\n","Building wheels for collected packages: allennlp\n","  Building wheel for allennlp (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for allennlp: filename=allennlp-0.9.1_unreleased-cp36-none-any.whl size=7535392 sha256=58fe0abddb93ac21b932a20ed98bef9e8a920de48bd73adc9ba42a390cd78629\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-tp2mjcqy/wheels/16/b3/9b/fceece1cbc3a6ac0c759db090cb239c3f4cba5bb369bb933c3\n","Successfully built allennlp\n","Building wheels for collected packages: ftfy, word2number, parsimonious, jsonnet, overrides\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=b677546102d46f0319b544f0bee2bf557bf0ede5fd56ad3abc4143ffd1410920\n","  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=73b701b13a040cd3c5f21d9e45ebda7b4c224e329fbe8393b9db0d20309ea620\n","  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n","  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=257291ec4232597e30a160f81dd64770f9aa9cbe73424ab7a7ee3c6c7a5b75e1\n","  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp36-cp36m-linux_x86_64.whl size=3387898 sha256=29cb6d3a2b27fab0d487edf98f6a0e77e05cdfe8f70846d1614e6436e0eec938\n","  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10174 sha256=974eafae32754eb61a169d77820c0422fd3548e8cf990281668943b36b270a4c\n","  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n","Successfully built ftfy word2number parsimonious jsonnet overrides\n","\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n","\u001b[31mERROR: botocore 1.19.30 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: responses 0.12.1 has requirement urllib3>=1.25.10, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: ftfy, conllu, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, word2number, parsimonious, jsonpickle, flask-cors, numpydoc, jsonnet, flaky, overrides, tensorboardX, unidecode, greenlet, zope.event, zope.interface, gevent, responses, preshed, plac, blis, thinc, spacy, sentencepiece, pytorch-transformers, allennlp\n","  Found existing installation: preshed 3.0.4\n","    Uninstalling preshed-3.0.4:\n","      Successfully uninstalled preshed-3.0.4\n","  Found existing installation: plac 1.1.3\n","    Uninstalling plac-1.1.3:\n","      Successfully uninstalled plac-1.1.3\n","  Found existing installation: blis 0.4.1\n","    Uninstalling blis-0.4.1:\n","      Successfully uninstalled blis-0.4.1\n","  Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed allennlp-0.9.1-unreleased blis-0.2.4 boto3-1.16.30 botocore-1.19.30 conllu-1.3.1 flaky-3.7.0 flask-cors-3.0.9 ftfy-5.8 gevent-20.9.0 greenlet-0.4.17 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-1.4.2 numpydoc-1.1.0 overrides-3.1.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.12.1 s3transfer-0.3.3 sentencepiece-0.1.94 spacy-2.1.9 tensorboardX-2.1 thinc-7.0.8 unidecode-1.1.1 word2number-1.1 zope.event-4.5.0 zope.interface-5.2.0\n","Collecting transformers==2.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n","\u001b[K     |████████████████████████████████| 481kB 13.7MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (2019.12.20)\n","Collecting tokenizers==0.0.11\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.1MB 28.7MB/s \n","\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (0.1.94)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (1.16.30)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 64.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (1.18.5)\n","Requirement already satisfied: botocore<1.20.0,>=1.19.30 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1) (1.19.30)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1) (0.10.0)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1) (0.3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.1) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.1) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.1) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1) (0.17.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.30->boto3->transformers==2.4.1) (2.8.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=d89600fd7725ab5d9363c205a697614e20b507e79c9c6d7eb4d1edbf5bcde68f\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.0.11 transformers-2.4.1\n","Collecting pytorch-transformers==1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n","\u001b[K     |████████████████████████████████| 184kB 13.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (1.18.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (2.23.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (1.7.0+cu101)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (2019.12.20)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (0.1.94)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (1.16.30)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (0.0.43)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (4.41.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (2020.11.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (1.24.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (0.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (3.7.4.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0) (0.10.0)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0) (0.3.3)\n","Requirement already satisfied: botocore<1.20.0,>=1.19.30 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0) (1.19.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (0.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (7.1.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.30->boto3->pytorch-transformers==1.2.0) (2.8.1)\n","\u001b[31mERROR: allennlp 0.9.1-unreleased has requirement pytorch-transformers==1.1.0, but you'll have pytorch-transformers 1.2.0 which is incompatible.\u001b[0m\n","Installing collected packages: pytorch-transformers\n","  Found existing installation: pytorch-transformers 1.1.0\n","    Uninstalling pytorch-transformers-1.1.0:\n","      Successfully uninstalled pytorch-transformers-1.1.0\n","Successfully installed pytorch-transformers-1.2.0\n","Cloning into 'dont-stop-pretraining'...\n","remote: Enumerating objects: 439, done.\u001b[K\n","remote: Total 439 (delta 0), reused 0 (delta 0), pack-reused 439\u001b[K\n","Receiving objects: 100% (439/439), 566.01 KiB | 1.46 MiB/s, done.\n","Resolving deltas: 100% (232/232), done.\n","Collecting adapter-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/44/1370c187aba1349d56d6813ec4de54644d15e154983050f4923ce5455069/adapter_transformers-1.1.0-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 13.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (1.18.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (0.0.43)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (4.41.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (3.12.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (0.8)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (20.4)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 51.3MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 52.9MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (2.23.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers) (0.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->adapter-transformers) (50.3.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->adapter-transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (2020.11.8)\n","\u001b[31mERROR: transformers 2.4.1 has requirement tokenizers==0.0.11, but you'll have tokenizers 0.9.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: allennlp 0.9.1-unreleased has requirement pytorch-transformers==1.1.0, but you'll have pytorch-transformers 1.2.0 which is incompatible.\u001b[0m\n","Installing collected packages: tokenizers, sentencepiece, adapter-transformers\n","  Found existing installation: tokenizers 0.0.11\n","    Uninstalling tokenizers-0.0.11:\n","      Successfully uninstalled tokenizers-0.0.11\n","  Found existing installation: sentencepiece 0.1.94\n","    Uninstalling sentencepiece-0.1.94:\n","      Successfully uninstalled sentencepiece-0.1.94\n","Successfully installed adapter-transformers-1.1.0 sentencepiece-0.1.91 tokenizers-0.9.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APoY5E_nppGl","executionInfo":{"status":"ok","timestamp":1607135251588,"user_tz":300,"elapsed":488,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"4349f6de-b618-4df4-c655-0b68fe445609"},"source":["!ls temp"],"execution_count":3,"outputs":[{"output_type":"stream","text":["ls: cannot access 'temp': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iLtp3PKXomSN","executionInfo":{"status":"ok","timestamp":1607135258941,"user_tz":300,"elapsed":1517,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"bb3a8eeb-e84d-4eb8-f97f-bc5f6dbda747"},"source":["!git clone https://github.com/nsusanj/temp.git"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Cloning into 'temp'...\n","remote: Enumerating objects: 24, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (24/24), done.\u001b[K\n","remote: Total 24 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (24/24), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wC5tYSmegrNX","executionInfo":{"status":"ok","timestamp":1607135279713,"user_tz":300,"elapsed":18664,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"3bf84fbb-6b99-41ee-cc79-6aba041a31fc"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ckv82WdJp3ul","executionInfo":{"status":"ok","timestamp":1607135303440,"user_tz":300,"elapsed":774,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"4e854306-3500-458b-d32a-446a79bb15b0"},"source":["!ls temp/adapticons"],"execution_count":6,"outputs":[{"output_type":"stream","text":["modeling  new_train_requirements.txt  README.md\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_iSdj3Img45R","executionInfo":{"status":"ok","timestamp":1607135310561,"user_tz":300,"elapsed":1002,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"87c2f1ec-9ba5-473d-e292-82cf11af70e1"},"source":["%cd temp/adapticons\n","%mkdir datasets\n","%cd datasets/\n","%mkdir citation\n","%cd citation"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons\n","/content/temp/adapticons/datasets\n","/content/temp/adapticons/datasets/citation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UcVwVlyKhAHd","executionInfo":{"status":"ok","timestamp":1607135316944,"user_tz":300,"elapsed":3930,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"93ed879d-6db4-443c-9540-18b8c46443c6"},"source":["!curl -Lo train.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/train.jsonl\n","!curl -Lo dev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/dev.jsonl\n","!curl -Lo test.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/test.jsonl"],"execution_count":8,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  454k  100  454k    0     0   290k      0  0:00:01  0:00:01 --:--:--  290k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 30138  100 30138    0     0  35125      0 --:--:-- --:--:-- --:--:-- 35084\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 38637  100 38637    0     0  44979      0 --:--:-- --:--:-- --:--:-- 44926\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dVtjFY2hDYH","executionInfo":{"status":"ok","timestamp":1607135322267,"user_tz":300,"elapsed":784,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"8bb48c95-24de-43de-d253-b33f02bd4589"},"source":["%cd ../../modeling"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/content/temp/adapticons/modeling\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z9ohHkOthF0F","executionInfo":{"status":"ok","timestamp":1607135324592,"user_tz":300,"elapsed":502,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"a2923818-c808-4042-cebe-699e1f13e42b"},"source":["!ls ../datasets/citation"],"execution_count":10,"outputs":[{"output_type":"stream","text":["dev.jsonl  test.jsonl  train.jsonl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7MRjgcOh7fD","executionInfo":{"status":"ok","timestamp":1607135327652,"user_tz":300,"elapsed":738,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"46a4d340-8232-4956-8394-8d26b29d8c5b"},"source":["!ls ../../../gdrive/MyDrive/downsample_sciee/_75_pct"],"execution_count":11,"outputs":[{"output_type":"stream","text":["checkpoint-1000  merges.txt\t\t  tokenizer_config.json\n","checkpoint-500\t pytorch_model.bin\t  training_args.bin\n","config.json\t special_tokens_map.json  vocab.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U-JbQlpNi8Xn","executionInfo":{"status":"ok","timestamp":1607048449536,"user_tz":300,"elapsed":73915,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}}},"source":["!cp -r ../../../gdrive/MyDrive/downsample_sciee/_75_pct ."],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XonwnGGZjE1X","executionInfo":{"status":"ok","timestamp":1607048455328,"user_tz":300,"elapsed":769,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"34fab1eb-0ad1-496b-afa1-50789a4b84a1"},"source":["!ls"],"execution_count":14,"outputs":[{"output_type":"stream","text":["_75_pct\t\t      new_train.py\n","download_glue.py      run_language_modeling_with_adapter_fusion.py\n","glue_training.py      run_language_modeling_with_adapters.py\n","jsonl_to_txt.py       runs\n","load_fusion_model.py  sample_pct_txt_file.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWwCnbohhJh4","executionInfo":{"status":"ok","timestamp":1607051212154,"user_tz":300,"elapsed":974460,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"6e1f1e65-fb1a-4270-fc04-7718f61efdd4"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/citation/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/downsample_newtrain/_75_tapt \\\n","  --task_name citation \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --model_name_or_path ../../../gdrive/MyDrive/downsample_sciee/_75_pct \\\n","  --metric macro"],"execution_count":16,"outputs":[{"output_type":"stream","text":["2020-12-04 02:50:41.073657: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/04/2020 02:50:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/04/2020 02:50:42 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/downsample_newtrain/_75_tapt', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec04_02-50-42_938389fdde6e', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/downsample_newtrain/_75_tapt', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_75_pct/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"citation\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_75_pct/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/downsample_sciee/_75_pct' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/downsample_sciee/_75_pct' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_75_pct/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_75_pct/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct/vocab.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_75_pct/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/downsample_sciee/_75_pct/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_75_pct were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_75_pct and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'citation' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}}.\n","***** Running training *****\n","  Num examples = 1688\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1060\n","{'loss': 0.691353271484375, 'learning_rate': 1.0566037735849058e-05, 'epoch': 4.716981132075472}\n","{'loss': 0.1267318115234375, 'learning_rate': 1.1320754716981133e-06, 'epoch': 9.433962264150944}\n","100% 1060/1060 [15:38<00:00,  1.29it/s]\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'epoch': 10.0}\n","100% 1060/1060 [15:38<00:00,  1.13it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/downsample_newtrain/_75_tapt\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_75_tapt/config.json\n","Model weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_75_tapt/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/04/2020 03:06:47 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 114\n","  Batch size = 8\n","100% 15/15 [00:02<00:00,  7.16it/s]\n","12/04/2020 03:06:49 - INFO - __main__ -   ***** Eval results citation *****\n","12/04/2020 03:06:49 - INFO - __main__ -     eval_loss = 0.9512717127799988\n","12/04/2020 03:06:49 - INFO - __main__ -     eval_accuracy = 0.8070175438596491\n","12/04/2020 03:06:49 - INFO - __main__ -     eval_f1 = 0.7053935923501141\n","12/04/2020 03:06:49 - INFO - __main__ -     eval_precision = 0.7117063492063492\n","12/04/2020 03:06:49 - INFO - __main__ -     eval_recall = 0.7345910680656443\n","12/04/2020 03:06:49 - INFO - __main__ -     epoch = 10.0\n","12/04/2020 03:06:49 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 139\n","  Batch size = 8\n","100% 18/18 [00:02<00:00,  7.03it/s]\n","12/04/2020 03:06:52 - INFO - __main__ -   ***** Test results citation *****\n","12/04/2020 03:06:52 - INFO - __main__ -     eval_loss = 1.1400259733200073\n","12/04/2020 03:06:52 - INFO - __main__ -     eval_accuracy = 0.7697841726618705\n","12/04/2020 03:06:52 - INFO - __main__ -     eval_f1 = 0.6647264859092142\n","12/04/2020 03:06:52 - INFO - __main__ -     eval_precision = 0.6562045260675397\n","12/04/2020 03:06:52 - INFO - __main__ -     eval_recall = 0.6795325800959603\n","12/04/2020 03:06:52 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gZ2iGziKiNU9","executionInfo":{"status":"ok","timestamp":1607135931564,"user_tz":300,"elapsed":572874,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"2d59d211-8ce6-47b2-862c-c2b21ceb7062"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/citation/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/downsample_newtrain/_50_tapt \\\n","  --task_name citation \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --model_name_or_path ../../../gdrive/MyDrive/downsample_sciee/_50_pct \\\n","  --metric macro"],"execution_count":12,"outputs":[{"output_type":"stream","text":["2020-12-05 02:29:21.236760: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/05/2020 02:29:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/05/2020 02:29:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/downsample_newtrain/_50_tapt', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec05_02-29-23_b1a909f9e801', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/downsample_newtrain/_50_tapt', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_50_pct/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"citation\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_50_pct/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/downsample_sciee/_50_pct' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/downsample_sciee/_50_pct' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_50_pct/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_50_pct/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct/vocab.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_50_pct/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/downsample_sciee/_50_pct/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_50_pct were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_50_pct and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'citation' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}}.\n","***** Running training *****\n","  Num examples = 1688\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1060\n","{'loss': 0.7187213134765625, 'learning_rate': 1.0566037735849058e-05, 'epoch': 4.716981132075472}\n","{'loss': 0.1504228515625, 'learning_rate': 1.1320754716981133e-06, 'epoch': 9.433962264150944}\n","100% 1060/1060 [08:51<00:00,  2.29it/s]\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'epoch': 10.0}\n","100% 1060/1060 [08:51<00:00,  1.99it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/downsample_newtrain/_50_tapt\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_50_tapt/config.json\n","Model weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_50_tapt/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/05/2020 02:38:48 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 114\n","  Batch size = 8\n","100% 15/15 [00:01<00:00, 13.26it/s]\n","12/05/2020 02:38:49 - INFO - __main__ -   ***** Eval results citation *****\n","12/05/2020 02:38:49 - INFO - __main__ -     eval_loss = 0.9482320547103882\n","12/05/2020 02:38:49 - INFO - __main__ -     eval_accuracy = 0.7982456140350878\n","12/05/2020 02:38:49 - INFO - __main__ -     eval_f1 = 0.6752264896422009\n","12/05/2020 02:38:49 - INFO - __main__ -     eval_precision = 0.6608199262611026\n","12/05/2020 02:38:49 - INFO - __main__ -     eval_recall = 0.7063693839117567\n","12/05/2020 02:38:49 - INFO - __main__ -     epoch = 10.0\n","12/05/2020 02:38:49 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 139\n","  Batch size = 8\n","100% 18/18 [00:01<00:00, 13.35it/s]\n","12/05/2020 02:38:50 - INFO - __main__ -   ***** Test results citation *****\n","12/05/2020 02:38:50 - INFO - __main__ -     eval_loss = 1.1959333419799805\n","12/05/2020 02:38:50 - INFO - __main__ -     eval_accuracy = 0.7697841726618705\n","12/05/2020 02:38:50 - INFO - __main__ -     eval_f1 = 0.6798652842431937\n","12/05/2020 02:38:50 - INFO - __main__ -     eval_precision = 0.7010847975553857\n","12/05/2020 02:38:50 - INFO - __main__ -     eval_recall = 0.6876582572357219\n","12/05/2020 02:38:50 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qg4mZGayu26a","executionInfo":{"status":"ok","timestamp":1607136659383,"user_tz":300,"elapsed":562558,"user":{"displayName":"Nathan Susanj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjoQndWkN5SDIjqyvu822oEPdtGmM_W1loHOWJD=s64","userId":"15663150816458790265"}},"outputId":"8c865952-e98b-42cd-c82d-1f3ab644e644"},"source":["!python new_train.py \\\n","  --do_train \\\n","  --do_eval \\\n","  --data_dir ../datasets/citation/ \\\n","  --max_seq_length 512 \\\n","  --per_device_train_batch_size 16 \\\n","  --gradient_accumulation_steps 1 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 10 \\\n","  --output_dir ../../../gdrive/MyDrive/downsample_newtrain/_25_tapt \\\n","  --task_name citation \\\n","  --do_predict \\\n","  --load_best_model_at_end \\\n","  --model_name_or_path ../../../gdrive/MyDrive/downsample_sciee/_25_pct \\\n","  --metric macro"],"execution_count":13,"outputs":[{"output_type":"stream","text":["2020-12-05 02:41:38.851898: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n","PyTorch: setting up devices\n","12/05/2020 02:41:40 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","12/05/2020 02:41:40 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='../../../gdrive/MyDrive/downsample_newtrain/_25_tapt', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec05_02-41-40_b1a909f9e801', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='../../../gdrive/MyDrive/downsample_newtrain/_25_tapt', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False)\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_25_pct/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"citation\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file ../../../gdrive/MyDrive/downsample_sciee/_25_pct/config.json\n","Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"adapters\": {\n","    \"adapters\": {},\n","    \"config_map\": {}\n","  },\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"type_vocab_size\": 1,\n","  \"vocab_size\": 50265\n","}\n","\n","Model name '../../../gdrive/MyDrive/downsample_sciee/_25_pct' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../../../gdrive/MyDrive/downsample_sciee/_25_pct' is a path, a model identifier, or url to a directory containing tokenizer files.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_25_pct/added_tokens.json. We won't load it.\n","Didn't find file ../../../gdrive/MyDrive/downsample_sciee/_25_pct/tokenizer.json. We won't load it.\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct/vocab.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct/merges.txt\n","loading file None\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct/special_tokens_map.json\n","loading file ../../../gdrive/MyDrive/downsample_sciee/_25_pct/tokenizer_config.json\n","loading file None\n","loading weights file ../../../gdrive/MyDrive/downsample_sciee/_25_pct/pytorch_model.bin\n","Some weights of the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_25_pct were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at ../../../gdrive/MyDrive/downsample_sciee/_25_pct and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding head 'citation' with config {'head_type': 'classification', 'num_labels': 6, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5}}.\n","***** Running training *****\n","  Num examples = 1688\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1060\n","{'loss': 0.7167046508789062, 'learning_rate': 1.0566037735849058e-05, 'epoch': 4.716981132075472}\n","{'loss': 0.14143682861328125, 'learning_rate': 1.1320754716981133e-06, 'epoch': 9.433962264150944}\n","100% 1060/1060 [08:51<00:00,  2.29it/s]\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'epoch': 10.0}\n","100% 1060/1060 [08:51<00:00,  1.99it/s]\n","Saving model checkpoint to ../../../gdrive/MyDrive/downsample_newtrain/_25_tapt\n","Configuration saved in ../../../gdrive/MyDrive/downsample_newtrain/_25_tapt/config.json\n","Model weights saved in ../../../gdrive/MyDrive/downsample_newtrain/_25_tapt/pytorch_model.bin\n","/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1201: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n","  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n","12/05/2020 02:50:55 - INFO - __main__ -   *** Evaluate ***\n","***** Running Evaluation *****\n","  Num examples = 114\n","  Batch size = 8\n","100% 15/15 [00:01<00:00, 13.33it/s]\n","12/05/2020 02:50:57 - INFO - __main__ -   ***** Eval results citation *****\n","12/05/2020 02:50:57 - INFO - __main__ -     eval_loss = 0.8932408094406128\n","12/05/2020 02:50:57 - INFO - __main__ -     eval_accuracy = 0.8070175438596491\n","12/05/2020 02:50:57 - INFO - __main__ -     eval_f1 = 0.7573960070112437\n","12/05/2020 02:50:57 - INFO - __main__ -     eval_precision = 0.7415937149270483\n","12/05/2020 02:50:57 - INFO - __main__ -     eval_recall = 0.8122746838848532\n","12/05/2020 02:50:57 - INFO - __main__ -     epoch = 10.0\n","12/05/2020 02:50:57 - INFO - root -   *** Test ***\n","***** Running Evaluation *****\n","  Num examples = 139\n","  Batch size = 8\n","100% 18/18 [00:01<00:00, 13.34it/s]\n","12/05/2020 02:50:58 - INFO - __main__ -   ***** Test results citation *****\n","12/05/2020 02:50:58 - INFO - __main__ -     eval_loss = 1.1109206676483154\n","12/05/2020 02:50:58 - INFO - __main__ -     eval_accuracy = 0.7985611510791367\n","12/05/2020 02:50:58 - INFO - __main__ -     eval_f1 = 0.6856723377949793\n","12/05/2020 02:50:58 - INFO - __main__ -     eval_precision = 0.7173669177093834\n","12/05/2020 02:50:58 - INFO - __main__ -     eval_recall = 0.6801614817107775\n","12/05/2020 02:50:58 - INFO - __main__ -     epoch = 10.0\n"],"name":"stdout"}]}]}